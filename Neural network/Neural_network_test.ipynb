{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Neural Network A Simple Perception**\n"
      ],
      "metadata": {
        "id": "Raqb5NhFJvIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is deep learning, and how is it connected to artificial intelligence\n"
      ],
      "metadata": {
        "id": "3gbvSUY3JkU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning is a subset of machine learning that involves using neural networks with many layers (hence \"deep\") to model complex patterns in large amounts of data. It has driven significant advancements in artificial intelligence (AI) because it allows machines to learn from data and make predictions or decisions without explicit programming.\n",
        "\n",
        "In simpler terms, deep learning enables AI to perform tasks like recognizing images, understanding speech, and playing games at or above human levels. Here’s a quick breakdown of their connection:\n",
        "\n",
        "**Artificial Intelligence (AI):** The broadest concept, referring to machines that can perform tasks that require human intelligence.\n",
        "\n",
        "**Machine Learning (ML):** A subset of AI that involves training algorithms on data to improve their performance over time.\n",
        "\n",
        "**Deep Learning (DL):** A subset of ML that uses neural networks with multiple layers to analyze various factors of data and learn from it in a way that's closer to how humans think.\n",
        "\n",
        "Deep learning's ability to handle and process vast amounts of data makes it pivotal for modern AI applications like autonomous vehicles, natural language processing, and recommendation systems. Think of it as the powerhouse behind many of the smart technologies we interact with today!"
      ],
      "metadata": {
        "id": "j4eC-eKwJka8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is a neural network, and what are the different types of neural networks?\n"
      ],
      "metadata": {
        "id": "XWKuxu69KXvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A neural network is a computational model inspired by the way biological neural networks in the human brain process information. These networks consist of interconnected layers of nodes, or \"neurons,\" which work together to recognize patterns, make decisions, and solve complex problems.\n",
        "\n",
        "Here's a breakdown of the different types of neural networks:\n",
        "\n",
        "1. **Feedforward Neural Networks (FNN):** The most basic type, where connections between nodes do not form a cycle. They are typically used for straightforward tasks like image recognition and classification.\n",
        "\n",
        "2. **Convolutional Neural Networks (CNN):** Particularly effective for image and video recognition tasks. They use convolutional layers to automatically and adaptively learn spatial hierarchies of features.\n",
        "\n",
        "3. **Recurrent Neural Networks (RNN):** Designed to recognize sequences of data, making them suitable for tasks like language modeling and time-series prediction. They have loops that allow information to persist, which is crucial for sequential data.\n",
        "\n",
        "4. **Long Short-Term Memory Networks (LSTM):** A type of RNN designed to remember information over long periods, solving the vanishing gradient problem. They're great for tasks like speech recognition and language translation.\n",
        "\n",
        "5. **Generative Adversarial Networks (GANs):** Consist of two neural networks, a generator and a discriminator, that compete against each other to produce realistic data. They're used for generating images, video content, and even music.\n",
        "\n",
        "6. **Autoencoders:** Used for unsupervised learning tasks. They consist of an encoder that compresses the input into a latent-space representation, and a decoder that reconstructs the input from this representation. They are useful for tasks like anomaly detection and data compression.\n",
        "\n",
        "7. **Transformers:** Particularly effective for natural language processing tasks. They rely on mechanisms known as attention to manage long-range dependencies in data, making them key in models like GPT (Generative Pre-trained Transformer).\n",
        "\n",
        "Each type of neural network has its strengths and is chosen based on the specific requirements of the task at hand. Together, they form the backbone of many cutting-edge AI applications."
      ],
      "metadata": {
        "id": "hsGTCEJLKZeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical structure of a neural network?\n"
      ],
      "metadata": {
        "id": "bJzd_VgkMEaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mathematical structure of a neural network is built around the concept of layers, neurons, and the connections (weights) between them. Here's a breakdown of the main components:\n",
        "\n",
        "### 1. Neurons\n",
        "- Each neuron is essentially a computational unit that takes in multiple inputs and produces a single output.\n",
        "- Mathematically, a neuron computes a weighted sum of its inputs and passes this sum through an activation function.\n",
        "\n",
        "### 2. Weights\n",
        "- Weights determine the strength and direction (positive or negative) of the connections between neurons. They are the parameters that the network learns during training.\n",
        "- Each connection from one neuron to another has an associated weight.\n",
        "\n",
        "### 3. Biases\n",
        "- Each neuron has an additional parameter called bias, which allows the activation function to be shifted left or right.\n",
        "- Biases help the network learn patterns that do not pass through the origin.\n",
        "\n",
        "### 4. Layers\n",
        "- **Input Layer:** The first layer of the network that receives the raw data.\n",
        "- **Hidden Layers:** Intermediate layers that perform computations and transformations on the inputs. There can be many hidden layers in a deep network.\n",
        "- **Output Layer:** The final layer that produces the network's output.\n",
        "\n",
        "### 5. Activation Functions\n",
        "- Activation functions introduce non-linearity into the network, allowing it to learn and represent complex patterns.\n",
        "- Common activation functions include:\n",
        "  - **Sigmoid:** \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\n",
        "  - **ReLU (Rectified Linear Unit):** \\(f(x) = \\max(0, x)\\)\n",
        "  - **Tanh (Hyperbolic Tangent):** \\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n",
        "\n",
        "### 6. Forward Propagation\n",
        "- The process of calculating the output of the network by passing the input data through the layers.\n",
        "- Mathematically, for a neuron \\(j\\) in layer \\(l\\):\n",
        "  \\(z_j^l = \\sum_{i} w_{ij}^l a_i^{(l-1)} + b_j^l\\)\n",
        "  where \\(z_j^l\\) is the weighted sum, \\(w_{ij}^l\\) are the weights, \\(a_i^{(l-1)}\\) are the activations from the previous layer, and \\(b_j^l\\) is the bias.\n",
        "  The output \\(a_j^l\\) is then:\n",
        "  \\(a_j^l = \\phi(z_j^l)\\)\n",
        "  where \\(\\phi\\) is the activation function.\n",
        "\n",
        "### 7. Backpropagation\n",
        "- The process of training the network by adjusting weights and biases to minimize the error between the predicted and actual outputs.\n",
        "- Involves calculating the gradient of the loss function with respect to each weight and bias, and updating them using gradient descent:\n",
        "  \\(\\Delta w_{ij} = -\\eta \\frac{\\partial L}{\\partial w_{ij}}\\)\n",
        "  where \\(\\Delta w_{ij}\\) is the change in weight, \\(\\eta\\) is the learning rate, \\(L\\) is the loss function, and \\(\\frac{\\partial L}{\\partial w_{ij}}\\) is the gradient.\n",
        "\n",
        "Here's a simple illustration of a single neuron:\n",
        "\n",
        "\\[\n",
        "\\begin{aligned}\n",
        "&\\text{Inputs: } x_1, x_2, \\ldots, x_n \\\\\n",
        "&\\text{Weights: } w_1, w_2, \\ldots, w_n \\\\\n",
        "&\\text{Bias: } b \\\\\n",
        "&\\text{Output: } y \\\\\n",
        "&\\text{Weighted Sum: } z = \\sum_{i=1}^{n} w_i x_i + b \\\\\n",
        "&\\text{Activation Function: } y = \\phi(z) \\\\\n",
        "\\end{aligned}\n",
        "\\]\n",
        "\n",
        "This structure scales up as you add more neurons and layers, creating the complex architectures needed for deep learning."
      ],
      "metadata": {
        "id": "0gT4L5UjMHxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is an activation function, and why is it essential in neural\n"
      ],
      "metadata": {
        "id": "ADgtiqBJM_Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An activation function in a neural network is a mathematical function applied to the output of each neuron. Its primary role is to introduce non-linearity into the network, allowing it to learn and model complex patterns in the data. Here's why activation functions are essential:\n",
        "\n",
        "### 1. Introducing Non-Linearity\n",
        "- Without an activation function, the neural network would behave like a linear regression model, regardless of the number of layers. It would only be able to solve linear problems, which are quite limited in scope.\n",
        "- Activation functions enable the network to capture and represent non-linear relationships, making it capable of handling a wide range of tasks, from image recognition to language processing.\n",
        "\n",
        "### 2. Controlling Output Range\n",
        "- Activation functions can control the range of the output values of a neuron, ensuring they remain within a specific boundary.\n",
        "- For example, the sigmoid function squashes the output to a range between 0 and 1, making it suitable for binary classification tasks.\n",
        "\n",
        "### 3. Ensuring Gradient Flow\n",
        "- Activation functions affect the gradients during backpropagation, which is critical for training the network.\n",
        "- Proper activation functions help mitigate issues like the vanishing gradient problem, where gradients become too small for the network to learn effectively.\n",
        "\n",
        "\n",
        "### Why Essential\n",
        "- **Learning Complex Patterns:** Enables the network to approximate complex functions by stacking multiple non-linear layers.\n",
        "- **Preventing Saturation:** Ensures that neurons do not get stuck during training, allowing the network to learn effectively.\n",
        "- **Improving Stability:** Helps in maintaining stability during training, particularly in deep networks."
      ],
      "metadata": {
        "id": "eEGeDI-WNB86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Could you list some common activation functions used in neural networks?\n"
      ],
      "metadata": {
        "id": "s-FmvD5GNSQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Common Activation Functions\n",
        "\n",
        "1. **Sigmoid**\n",
        "   \\[\n",
        "   \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "   \\]\n",
        "   - Range: (0, 1)\n",
        "   - Commonly used in binary classification tasks.\n",
        "\n",
        "2. **ReLU (Rectified Linear Unit)**\n",
        "   \\[\n",
        "   f(x) = \\max(0, x)\n",
        "   \\]\n",
        "   - Range: [0, ∞)\n",
        "   - Popular due to its simplicity and effectiveness in deep networks.\n",
        "\n",
        "3. **Tanh (Hyperbolic Tangent)**\n",
        "   \\[\n",
        "   \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "   \\]\n",
        "   - Range: (-1, 1)\n",
        "   - Often used in tasks where the output needs to be centered around zero.\n",
        "\n",
        "4. **Leaky ReLU**\n",
        "   \\[\n",
        "   f(x) = \\begin{cases}\n",
        "   x & \\text{if } x > 0 \\\\\n",
        "   \\alpha x & \\text{if } x \\leq 0\n",
        "   \\end{cases}\n",
        "   \\]\n",
        "   - Similar to ReLU but with a small slope for negative values of \\(x\\), preventing the neuron from dying during training.\n",
        "\n",
        "5. **Softmax**\n",
        "   \\[\n",
        "   \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}\n",
        "   \\]\n",
        "   - Range: (0, 1) for each class, with the sum equal to 1\n",
        "   - Used in the output layer of classification networks to represent class probabilities."
      ],
      "metadata": {
        "id": "eeIqRpX4NY_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a multilayer neural network?\n"
      ],
      "metadata": {
        "id": "XudcrUMZNbmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A multilayer neural network, also known as a multi-layer perceptron (MLP), is a type of artificial neural network that consists of multiple layers of neurons. These layers work together to learn and make predictions or classifications based on the input data. Here's a breakdown of its structure:\n",
        "\n",
        "### Structure of a Multilayer Neural Network\n",
        "\n",
        "1. **Input Layer**\n",
        "   - The first layer that receives the raw input data.\n",
        "   - Each neuron in this layer represents a feature from the input data.\n",
        "\n",
        "2. **Hidden Layers**\n",
        "   - Intermediate layers between the input and output layers.\n",
        "   - Consist of multiple neurons, and there can be one or more hidden layers.\n",
        "   - Each neuron in these layers takes input from the previous layer, applies weights, biases, and activation functions, and sends the result to the next layer.\n",
        "   - The more hidden layers there are, the \"deeper\" the network, which is why they are also called \"deep neural networks\" when multiple hidden layers are involved.\n",
        "\n",
        "3. **Output Layer**\n",
        "   - The final layer that produces the network's output.\n",
        "   - The number of neurons in this layer depends on the nature of the task (e.g., one neuron for binary classification, multiple neurons for multi-class classification).\n",
        "\n",
        "### Working Principle\n",
        "\n",
        "1. **Forward Propagation**\n",
        "   - The process of passing input data through the layers of the network.\n",
        "   - Each neuron in the network performs a weighted sum of its inputs, adds a bias term, and applies an activation function to produce its output.\n",
        "   - The outputs of neurons in one layer become the inputs for neurons in the next layer.\n",
        "\n",
        "2. **Activation Functions**\n",
        "   - Non-linear functions applied to the weighted sums at each neuron.\n",
        "   - Common activation functions include ReLU, Sigmoid, and Tanh, which introduce non-linearity, allowing the network to model complex relationships.\n",
        "\n",
        "3. **Backpropagation**\n",
        "   - A training method that adjusts the weights and biases in the network to minimize the error between the predicted and actual outputs.\n",
        "   - Involves calculating the gradient of the loss function with respect to each weight and bias, then updating them using gradient descent.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- **Learning Complex Patterns:** Can model highly non-linear and complex relationships in data.\n",
        "- **Flexibility:** Can be used for a variety of tasks, including classification, regression, and feature extraction.\n",
        "\n",
        "### Example Architecture\n",
        "\n",
        "A simple example of a multilayer neural network architecture could look like this:\n",
        "- **Input Layer:** 3 neurons (for a dataset with 3 features)\n",
        "- **Hidden Layer 1:** 5 neurons\n",
        "- **Hidden Layer 2:** 4 neurons\n",
        "- **Output Layer:** 1 neuron (for binary classification)\n",
        "\n",
        "Here’s a visual representation:\n",
        "\n",
        "```\n",
        "Input Layer --> Hidden Layer 1 --> Hidden Layer 2 --> Output Layer\n",
        "   (3 neurons)    (5 neurons)        (4 neurons)         (1 neuron)\n",
        "```\n",
        "\n",
        "This structure allows the network to learn complex patterns and make predictions based on the input data."
      ],
      "metadata": {
        "id": "LHyVCiOuNgwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is a loss function, and why is it crucial for neural network training?\n"
      ],
      "metadata": {
        "id": "ncHuSJ6fNwos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A loss function, also known as a cost function or objective function, measures the difference between the predicted outputs of a neural network and the actual target values. It quantifies the error or the \"loss\" of the model, guiding the training process by indicating how well or poorly the model is performing.\n",
        "\n",
        "### Why a Loss Function is Crucial:\n",
        "\n",
        "1. **Guiding the Learning Process**\n",
        "   - During training, the neural network uses the loss function to update its weights and biases. By minimizing the loss function, the network improves its accuracy.\n",
        "   - The network adjusts its parameters to reduce the difference between the predicted and actual values.\n",
        "\n",
        "2. **Evaluating Performance**\n",
        "   - The value of the loss function gives a clear indication of the network's performance.\n",
        "   - A lower loss value typically means better model performance, while a higher loss indicates more errors.\n",
        "\n",
        "3. **Optimizing the Network**\n",
        "   - The loss function is used in conjunction with optimization algorithms, like gradient descent, to find the optimal set of weights and biases.\n",
        "   - The gradients of the loss function with respect to the network's parameters are computed, and these gradients guide the parameter updates.\n"
      ],
      "metadata": {
        "id": "qtIYebL7N0N6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What are some common types of loss functions?\n"
      ],
      "metadata": {
        "id": "LV51jzlzN_2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Common Loss Functions\n",
        "\n",
        "1. **Mean Squared Error (MSE)**\n",
        "   \\[\n",
        "   \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "   \\]\n",
        "   - Used for regression tasks where the goal is to predict continuous values.\n",
        "   - Measures the average squared difference between the actual and predicted values.\n",
        "\n",
        "2. **Cross-Entropy Loss (Log Loss)**\n",
        "   \\[\n",
        "   \\text{Cross-Entropy} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
        "   \\]\n",
        "   - Commonly used for classification tasks.\n",
        "   - Measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
        "\n",
        "3. **Categorical Cross-Entropy**\n",
        "   \\[\n",
        "   \\text{Categorical Cross-Entropy} = -\\sum_{i} y_i \\log(\\hat{y}_i)\n",
        "   \\]\n",
        "   - Used for multi-class classification problems.\n",
        "   - Compares the predicted probability distribution across multiple classes with the actual distribution.\n",
        "\n",
        "4. **Binary Cross-Entropy**\n",
        "   \\[\n",
        "   \\text{Binary Cross-Entropy} = - \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
        "   \\]\n",
        "   - Specific to binary classification tasks.\n",
        "\n",
        "5. **Hinge Loss**\n",
        "   \\[\n",
        "   \\text{Hinge Loss} = \\sum_{i} \\max(0, 1 - y_i \\cdot \\hat{y}_i)\n",
        "   \\]\n",
        "   - Commonly used for training support vector machines.\n",
        "   - Ensures that the predictions are not only correct but also at a margin from the decision boundary.\n",
        "\n",
        "The choice of loss function depends on the nature of the problem you're solving. It plays a vital role in how effectively the neural network can learn and generalize from the training data to new, unseen data."
      ],
      "metadata": {
        "id": "sWBGF_NqOED4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How does a neural network learn?\n"
      ],
      "metadata": {
        "id": "8NNs7cf3OHdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A neural network learns through a process called training, which involves adjusting its weights and biases to minimize the error in its predictions. Here's a simplified breakdown of how this learning process works:\n",
        "\n",
        "### 1. Initialization\n",
        "- The network starts with random weights and biases.\n",
        "\n",
        "### 2. Forward Propagation\n",
        "- Input data is passed through the network from the input layer to the output layer.\n",
        "- Each neuron performs a weighted sum of its inputs, adds a bias term, and applies an activation function to produce its output.\n",
        "- This process continues through all the layers, resulting in the network's final prediction.\n",
        "\n",
        "### 3. Loss Calculation\n",
        "- The network's prediction is compared to the actual target value using a loss function.\n",
        "- The loss function calculates the difference (or error) between the predicted output and the actual output.\n",
        "\n",
        "### 4. Backpropagation\n",
        "- The backpropagation algorithm computes the gradient (partial derivatives) of the loss function with respect to each weight and bias in the network.\n",
        "- This involves the chain rule of calculus to propagate the error backward through the network, from the output layer to the input layer.\n",
        "\n",
        "### 5. Weight and Bias Updates\n",
        "- Using the gradients calculated during backpropagation, the network updates its weights and biases to minimize the error.\n",
        "- This is done using an optimization algorithm like gradient descent, which adjusts the weights and biases in the direction that reduces the loss.\n",
        "- The update rule for a weight \\( w \\) is typically:\n",
        "  \\[\n",
        "  w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\n",
        "  \\]\n",
        "  where \\( \\eta \\) is the learning rate, and \\( \\frac{\\partial L}{\\partial w} \\) is the gradient of the loss function with respect to the weight.\n",
        "\n",
        "### 6. Iteration and Convergence\n",
        "- The process of forward propagation, loss calculation, backpropagation, and weight update is repeated for multiple iterations (epochs) over the training dataset.\n",
        "- The network gradually learns the underlying patterns in the data, and the loss decreases over time.\n",
        "- Training continues until the network reaches an acceptable level of accuracy or the loss function converges (i.e., further training doesn't significantly reduce the loss).\n",
        "\n",
        "### Simplified Example\n",
        "Imagine training a neural network to recognize handwritten digits (like in the MNIST dataset):\n",
        "- **Forward Propagation:** Input an image of a digit (e.g., '5'), pass it through the network, and get a prediction (e.g., the network predicts '3').\n",
        "- **Loss Calculation:** Compare the predicted digit ('3') to the actual digit ('5') and calculate the error.\n",
        "- **Backpropagation:** Compute the gradients of the loss with respect to each weight and bias.\n",
        "- **Weight Updates:** Adjust the weights and biases to reduce the error.\n",
        "- **Iteration:** Repeat the process with many images until the network correctly recognizes handwritten digits.\n",
        "\n",
        "This learning process is what enables neural networks to become powerful tools for tasks like image recognition, language processing, and much more."
      ],
      "metadata": {
        "id": "rUQN8nL8OS03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is an optimizer in neural networks, and why is it necessary?\n"
      ],
      "metadata": {
        "id": "2O5FAbLdOZHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer in neural networks is an algorithm that adjusts the network’s weights and biases to minimize the loss function. Its primary role is to guide the network toward better performance by finding the optimal set of parameters that reduce the error between the predicted outputs and the actual target values.\n",
        "\n",
        "### Why Optimizers are Necessary:\n",
        "\n",
        "1. **Efficient Learning:**\n",
        "   - Optimizers efficiently navigate the high-dimensional space of neural network parameters, accelerating the learning process.\n",
        "   - They ensure that the network converges to a minimum (ideally a global minimum) of the loss function, improving accuracy and performance.\n",
        "\n",
        "2. **Handling Complex Architectures:**\n",
        "   - Neural networks, especially deep ones, have millions of parameters. Manually adjusting these parameters would be impractical.\n",
        "   - Optimizers automate this process, dynamically updating the parameters during training.\n",
        "\n",
        "3. **Improving Generalization:**\n",
        "   - By minimizing the loss function effectively, optimizers help the network generalize better to new, unseen data, reducing overfitting and improving robustness."
      ],
      "metadata": {
        "id": "QdYCeG-COlBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Could you briefly describe some common optimizers?\n"
      ],
      "metadata": {
        "id": "HuZBJCYbPLpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Optimization Algorithms:\n",
        "\n",
        "1. **Gradient Descent:**\n",
        "   \\[\n",
        "   w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\n",
        "   \\]\n",
        "   - Updates weights by moving them in the direction of the negative gradient of the loss function.\n",
        "   - Simple and effective but can be slow for large datasets.\n",
        "\n",
        "2. **Stochastic Gradient Descent (SGD):**\n",
        "   - Uses a single randomly chosen example (or a small batch) to compute the gradient, which reduces computational burden and often improves convergence speed.\n",
        "   - Introduces noise into the gradient, helping escape local minima.\n",
        "\n",
        "3. **Mini-Batch Gradient Descent:**\n",
        "   - A compromise between batch and stochastic gradient descent, using a subset (mini-batch) of the training data to compute the gradient.\n",
        "   - Balances computational efficiency and convergence speed.\n",
        "\n",
        "4. **Momentum:**\n",
        "   \\[\n",
        "   v_t = \\gamma v_{t-1} + \\eta \\frac{\\partial L}{\\partial w}\n",
        "   \\]\n",
        "   \\[\n",
        "   w \\leftarrow w - v_t\n",
        "   \\]\n",
        "   - Adds a fraction of the previous update to the current update, helping to accelerate convergence and reduce oscillations.\n",
        "\n",
        "5. **Adaptive Learning Rate Methods:**\n",
        "   - **AdaGrad:** Adjusts the learning rate based on the historical gradient, allowing larger updates for less frequently updated parameters.\n",
        "   - **RMSprop:** Maintains a moving average of the squared gradients, normalizing the updates to deal with the diminishing learning rate problem.\n",
        "   - **Adam (Adaptive Moment Estimation):** Combines the benefits of RMSprop and Momentum, using adaptive learning rates and storing exponentially decaying averages of past gradients and squared gradients.\n",
        "\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Optimizers are crucial for efficiently training neural networks, ensuring that they learn effectively from the data and perform well on new, unseen data. They automate the process of parameter updates, making it possible to train complex models in a feasible amount of time.\n",
        "\n",
        "Would you like to know more about any specific optimizer or dive deeper into how they work?"
      ],
      "metadata": {
        "id": "UpOJlVMUPPdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Can you explain forward and backward propagation in a neural network?\n"
      ],
      "metadata": {
        "id": "oPy0uEjhPbX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward and backward propagation are key concepts in training a neural network.\n",
        "\n",
        "\n",
        "### **1. Forward Propagation:**\n",
        "Forward propagation refers to the process of passing input data through the network to compute the output (or predictions).\n",
        "\n",
        "#### Steps:\n",
        "1. **Input Layer:**\n",
        "   - Input data is provided to the neural network.\n",
        "\n",
        "2. **Weighted Sum and Activation:**\n",
        "   - For each neuron, the input values are multiplied by their respective weights, and a bias is added:\n",
        "     \\[\n",
        "     z = w \\cdot x + b\n",
        "     \\]\n",
        "   - An activation function (e.g., ReLU, sigmoid, tanh) is applied to the weighted sum to introduce non-linearity:\n",
        "     \\[\n",
        "     a = \\text{activation}(z)\n",
        "     \\]\n",
        "\n",
        "3. **Layer-by-Layer Computation:**\n",
        "   - The activations from one layer are passed as inputs to the next layer.\n",
        "   - This process continues until the final output layer produces predictions.\n",
        "\n",
        "4. **Output:**\n",
        "   - The final layer produces the network's output, often processed through an activation function (e.g., softmax for classification).\n",
        "\n",
        "#### Purpose:\n",
        "- Forward propagation computes the predicted output based on current weights and biases, which is later used to evaluate the loss.\n",
        "\n",
        "\n",
        "### **2. Backward Propagation:**\n",
        "Backward propagation, or backpropagation, is the process of updating the weights and biases in the network to minimize the error (loss).\n",
        "\n",
        "#### Steps:\n",
        "1. **Compute Loss:**\n",
        "   - The predicted output from forward propagation is compared with the actual target values using a loss function (e.g., Mean Squared Error, Cross-Entropy Loss).\n",
        "\n",
        "2. **Calculate Gradients:**\n",
        "   - Using the chain rule of calculus, gradients of the loss with respect to weights and biases are computed layer-by-layer, starting from the output layer and moving backward.\n",
        "\n",
        "   For each layer:\n",
        "   - The gradient of the loss with respect to the output of a neuron is calculated.\n",
        "   - This is propagated backward to compute gradients with respect to the weights and biases.\n",
        "\n",
        "3. **Update Parameters:**\n",
        "   - Gradients are used to update the weights and biases using an optimization algorithm (e.g., Gradient Descent, Adam):\n",
        "     \\[\n",
        "     w \\gets w - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
        "     \\]\n",
        "     \\[\n",
        "     b \\gets b - \\eta \\cdot \\frac{\\partial L}{\\partial b}\n",
        "     \\]\n",
        "     Here, \\( \\eta \\) is the learning rate.\n",
        "\n",
        "4. **Repeat:**\n",
        "   - Forward and backward propagation are repeated for multiple iterations (epochs) until the loss converges or meets a stopping criterion.\n",
        "\n",
        "#### Purpose:\n",
        "- Backward propagation ensures that the network learns by adjusting the parameters to reduce the loss.\n",
        "\n",
        "\n",
        "\n",
        "### **Key Relationship Between Forward and Backward Propagation:**\n",
        "- **Forward Propagation:** Computes the predictions.\n",
        "- **Backward Propagation:** Updates the model's parameters to improve future predictions.\n",
        "\n",
        "Together, these steps allow the network to learn from data iteratively."
      ],
      "metadata": {
        "id": "Yv3qqGQZPi8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is weight initialization, and how does it impact training?\n"
      ],
      "metadata": {
        "id": "766WFUJvQg8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight initialization refers to the process of setting the initial values for the weights in a neural network before training begins. The choice of these initial values can significantly impact how well and how quickly the network learns. Here’s why it’s important and how it affects training:\n",
        "\n",
        "### Why Weight Initialization Matters:\n",
        "\n",
        "1. **Avoiding Symmetry:**\n",
        "   - If all weights are initialized to the same value (e.g., zero), the neurons in each layer will learn the same features during training, rendering the network ineffective.\n",
        "   - Proper initialization breaks this symmetry, allowing different neurons to learn different features.\n",
        "\n",
        "2. **Controlling Signal Variance:**\n",
        "   - Poor initialization can cause the activations to either explode (grow too large) or vanish (shrink too small), hindering the learning process.\n",
        "   - Good initialization methods ensure that the activations and gradients maintain appropriate variances throughout the network layers.\n",
        "\n",
        "3. **Speeding Up Convergence:**\n",
        "   - Properly initialized weights can help the network converge faster by starting the learning process closer to the optimal solution.\n",
        "   - This reduces the number of training epochs needed and improves computational efficiency.\n",
        "\n",
        "### Common Initialization Methods:\n",
        "\n",
        "1. **Zero Initialization:**\n",
        "   - Setting all weights to zero.\n",
        "   - Not recommended because it leads to symmetry and prevents effective learning.\n",
        "\n",
        "2. **Random Initialization:**\n",
        "   - Initializing weights randomly using a normal (Gaussian) or uniform distribution.\n",
        "   - Helps in breaking symmetry, but if not scaled properly, can still lead to exploding or vanishing gradients.\n",
        "\n",
        "3. **Xavier/Glorot Initialization:**\n",
        "   \\[\n",
        "   W \\sim \\mathcal{U}\\left(-\\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}}\\right)\n",
        "   \\]\n",
        "   - Proposed by Glorot and Bengio, it’s designed to keep the variance of activations and gradients roughly the same across all layers.\n",
        "   - Suitable for networks with sigmoid or tanh activation functions.\n",
        "\n",
        "4. **He Initialization:**\n",
        "   \\[\n",
        "   W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in}}\\right)\n",
        "   \\]\n",
        "   - Proposed by He et al., it’s similar to Xavier but scaled to work well with ReLU activation functions.\n",
        "   - Ensures that the activations and gradients maintain appropriate variances.\n",
        "\n",
        "5. **LeCun Initialization:**\n",
        "   \\[\n",
        "   W \\sim \\mathcal{N}\\left(0, \\frac{1}{n_{in}}\\right)\n",
        "   \\]\n",
        "   - Proposed by LeCun, it’s particularly suitable for networks using the SELU activation function.\n",
        "\n",
        "### Impact on Training:\n",
        "\n",
        "- **Training Stability:** Proper initialization helps in maintaining stable gradients, avoiding issues like exploding or vanishing gradients.\n",
        "- **Learning Speed:** Good initial weights can lead to faster convergence, reducing the overall training time.\n",
        "- **Model Performance:** Proper initialization can lead to better generalization and improved model accuracy.\n",
        "\n",
        "In summary, weight initialization plays a crucial role in the effective training of neural networks. Choosing an appropriate initialization method tailored to the specific architecture and activation functions can significantly enhance the network’s performance and efficiency.\n"
      ],
      "metadata": {
        "id": "q0tRkD83QlBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the vanishing gradient problem in deep learning?\n"
      ],
      "metadata": {
        "id": "kUuBHHMMQ5Zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vanishing gradient problem is a challenge encountered during the training of deep neural networks, particularly those with many layers, such as Recurrent Neural Networks (RNNs) and deep feedforward networks. It occurs when the gradients used to update the network's weights become very small, effectively \"vanishing,\" which hinders the learning process. Here’s a breakdown of why this happens and its impact:\n",
        "\n",
        "### Why the Vanishing Gradient Problem Occurs\n",
        "\n",
        "1. **Chain Rule of Derivatives:**\n",
        "   - During backpropagation, the gradients are calculated using the chain rule of derivatives. This involves multiplying the gradients of the loss function with respect to the network's weights through each layer.\n",
        "   - When the network has many layers, this multiplication can lead to very small values, especially if the activation functions used have derivatives less than 1 (like sigmoid or tanh).\n",
        "\n",
        "2. **Activation Functions:**\n",
        "   - Sigmoid and tanh functions can squash their inputs into a small range of values, where the gradient can be very close to zero.\n",
        "   - As gradients are backpropagated through these layers, they can shrink exponentially, leading to very small updates for the weights in the earlier layers.\n",
        "\n",
        "3. **Depth of the Network:**\n",
        "   - The deeper the network, the more layers the gradients must pass through, increasing the likelihood of them diminishing to near zero.\n",
        "\n",
        "### Impact on Training\n",
        "\n",
        "- **Slow Convergence:**\n",
        "  - If the gradients become too small, the weights in the earlier layers are updated very slowly, leading to slow convergence of the network.\n",
        "  - The network takes a long time to learn useful patterns from the data.\n",
        "\n",
        "- **Poor Learning:**\n",
        "  - In extreme cases, the network may stop learning altogether, as the weights remain almost unchanged after many iterations.\n",
        "  - This can result in the network not being able to capture the necessary features for accurate predictions.\n",
        "\n",
        "### Mitigation Techniques\n",
        "\n",
        "1. **Using Different Activation Functions:**\n",
        "   - **ReLU (Rectified Linear Unit):** Unlike sigmoid and tanh, ReLU does not suffer as much from the vanishing gradient problem because its derivative is either 0 or 1.\n",
        "   - **Leaky ReLU:** A variant of ReLU that allows a small, non-zero gradient when the unit is not active.\n",
        "\n",
        "2. **Weight Initialization:**\n",
        "   - **Xavier/Glorot Initialization:** Helps maintain the variance of gradients throughout the layers.\n",
        "   - **He Initialization:** Designed for ReLU and its variants to keep gradients in a useful range.\n",
        "\n",
        "3. **Batch Normalization:**\n",
        "   - Normalizes the inputs of each layer, ensuring that they have a consistent distribution, which helps mitigate the vanishing gradient problem.\n",
        "   - Improves the stability and speed of the training process.\n",
        "\n",
        "4. **Residual Networks (ResNets):**\n",
        "   - Introduces shortcut connections that allow gradients to flow directly through the network, bypassing some layers, which helps in mitigating the vanishing gradient problem.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The vanishing gradient problem is a significant challenge in training deep neural networks, but with the right techniques and practices, its impact can be mitigated. Understanding and addressing this problem is crucial for building effective deep learning models that learn efficiently and perform well on complex tasks.\n"
      ],
      "metadata": {
        "id": "TWZd-4s7RBaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is the exploding gradient problem?\n"
      ],
      "metadata": {
        "id": "BRCd73pnRI09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exploding gradient problem is a phenomenon where the gradients used to update the weights of a neural network become excessively large during training. This can lead to extremely large updates to the weights, causing the network to become unstable and making it difficult or impossible to learn effectively. Here’s why this happens and its impact:\n",
        "\n",
        "### Why the Exploding Gradient Problem Occurs\n",
        "\n",
        "1. **Chain Rule of Derivatives:**\n",
        "   - During backpropagation, gradients are computed using the chain rule, which involves multiplying the derivatives layer by layer.\n",
        "   - If the weights are too large or the model architecture is not well-designed, this multiplication can lead to gradients that grow exponentially through the layers.\n",
        "\n",
        "2. **Deep Networks:**\n",
        "   - In very deep networks with many layers, the repeated multiplication of gradients can result in values that increase rapidly, leading to exploding gradients.\n",
        "\n",
        "### Impact on Training\n",
        "\n",
        "- **Instability:**\n",
        "  - The network becomes unstable with excessively large weight updates, which can cause the loss function to oscillate or diverge.\n",
        "  - The training process may fail to converge, resulting in poor model performance.\n",
        "\n",
        "- **Poor Generalization:**\n",
        "  - Even if the network manages to converge, the solutions found might not generalize well to new data due to instability during training.\n",
        "  - This often leads to overfitting or suboptimal learning.\n",
        "\n",
        "### Mitigation Techniques\n",
        "\n",
        "1. **Gradient Clipping:**\n",
        "   - A popular technique to mitigate exploding gradients involves clipping the gradients during backpropagation.\n",
        "   - This means setting a threshold value and scaling down the gradients that exceed this threshold to maintain stability.\n",
        "   - For example, if the gradient norm exceeds a predefined threshold, it is scaled back to match this threshold.\n",
        "\n",
        "2. **Weight Regularization:**\n",
        "   - Techniques like L2 regularization can penalize large weights and help in preventing the gradients from exploding.\n",
        "   - Regularization adds a penalty term to the loss function, encouraging the network to keep weights smaller.\n",
        "\n",
        "3. **Proper Initialization:**\n",
        "   - Initializing weights carefully using methods like He initialization or Xavier initialization can help maintain gradients at appropriate levels.\n",
        "   - Proper initialization ensures that the variance of gradients is controlled throughout the layers.\n",
        "\n",
        "4. **Smaller Learning Rates:**\n",
        "   - Using smaller learning rates can help in preventing large updates to the weights.\n",
        "   - While it may slow down training, it often helps in maintaining stability, especially in the early stages of training.\n",
        "\n",
        "5. **Batch Normalization:**\n",
        "   - Normalizing the inputs of each layer helps in controlling the variance of the inputs, which can mitigate both exploding and vanishing gradient problems.\n",
        "   - Batch normalization ensures that the inputs to each layer have a stable distribution, improving training stability.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The exploding gradient problem is a critical challenge in training deep neural networks, but with the right techniques and practices, its impact can be mitigated. Understanding and addressing this problem is essential for building effective deep learning models that can learn efficiently and perform well on complex tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xmy16ss4RReE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "KqOJ6Y-CRydi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "bVNO501SR2kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "Si_x7-FnR22K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "OBwxIHu9R4Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical**"
      ],
      "metadata": {
        "id": "aXjIH328R569"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How do you create a simple perceptron for basic binary classification?\n"
      ],
      "metadata": {
        "id": "EQ1zoerNSAqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation function (Step function)\n",
        "def step_function(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Perceptron class\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, learning_rate=0.1):\n",
        "        self.weights = np.zeros(input_size)\n",
        "        self.bias = 0\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute the weighted sum of inputs\n",
        "        summation = np.dot(X, self.weights) + self.bias\n",
        "        return step_function(summation)\n",
        "\n",
        "    def train(self, X, y, epochs=100):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(len(X)):\n",
        "                # Predict the output for the given input\n",
        "                prediction = self.predict(X[i])\n",
        "                # Update the weights if prediction is wrong\n",
        "                error = y[i] - prediction\n",
        "                self.weights += self.learning_rate * error * X[i]\n",
        "                self.bias += self.learning_rate * error\n",
        "\n",
        "# Example data for AND gate (binary classification)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 0, 0, 1])  # AND gate labels\n",
        "\n",
        "# Initialize and train perceptron\n",
        "perceptron = Perceptron(input_size=2, learning_rate=0.1)\n",
        "perceptron.train(X, y, epochs=10)\n",
        "\n",
        "# Test the perceptron\n",
        "for input_data in X:\n",
        "    print(f\"Input: {input_data} Prediction: {perceptron.predict(input_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsRpDzRFSCHP",
        "outputId": "da4c8e6e-d9eb-4567-f48a-ff47ae91edf9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0] Prediction: 0\n",
            "Input: [0 1] Prediction: 0\n",
            "Input: [1 0] Prediction: 0\n",
            "Input: [1 1] Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How can you build a neural network with one hidden layer using Keras?\n"
      ],
      "metadata": {
        "id": "fIPbI4cUSsh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset (input features and target labels)\n",
        "# In practice, you would load real data here\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Example input (XOR problem)\n",
        "y = np.array([0, 1, 1, 0])  # Example output\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add input layer and hidden layer\n",
        "# The input shape should match the number of features in your dataset\n",
        "model.add(Dense(units=8, input_dim=2, activation='relu'))  # Hidden layer with 8 neurons\n",
        "\n",
        "# Add output layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # Sigmoid for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=4)\n",
        "\n",
        "# Evaluate the model (optional)\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJXg-9EGTUw_",
        "outputId": "7892b8e7-979d-41d2-a1d1-4d1d8a530d29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952ms/step - accuracy: 0.5000 - loss: 0.6990\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.2500 - loss: 0.6986\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6982\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6977\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6973\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6968\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6964\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6959\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6954\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6950\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6946\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6941\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6937\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6928\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6923\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6919\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6915\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6910\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6906\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 0.6901\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6897\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6893\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6888\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6884\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6879\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 0.6875\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5000 - loss: 0.6871\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5000 - loss: 0.6866\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.6862\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.6858\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5000 - loss: 0.6854\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6850\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6846\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6843\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6839\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6835\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6831\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6828\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6824\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6820\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.6817\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6813\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6809\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5000 - loss: 0.6806\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.6802\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6799\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6795\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6791\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 0.6788\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.6784\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5000 - loss: 0.6781\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.6777\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6774\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.6770\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.6767\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6763\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6760\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6756\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6753\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6749\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6746\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6742\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6739\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6735\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6732\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6728\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6725\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6722\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6718\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6715\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6711\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6708\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6705\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6701\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.6698\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 0.6694\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6691\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.6688\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6684\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6681\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6678\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6674\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6671\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6668\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6664\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.6661\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6658\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6655\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6651\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6648\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6645\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6642\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 0.6638\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5000 - loss: 0.6635\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6632\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6629\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6625\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5000 - loss: 0.6622\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6619\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.5000 - loss: 0.6616\n",
            "Loss: 0.6615704298019409, Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. How do you initialize weights using the Xavier (Glorot) initialization method in Keras?"
      ],
      "metadata": {
        "id": "M8ElxPd-SuSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.initializers import GlorotUniform\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset (input features and target labels)\n",
        "# In practice, you would load real data here\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Example input (XOR problem)\n",
        "y = np.array([0, 1, 1, 0])  # Example output\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add input layer and hidden layer with Xavier initialization\n",
        "model.add(Dense(units=8, input_dim=2, activation='relu', kernel_initializer=GlorotUniform()))\n",
        "\n",
        "# Add output layer\n",
        "model.add(Dense(units=1, activation='sigmoid', kernel_initializer=GlorotUniform()))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=4)\n",
        "\n",
        "# Evaluate the model (optional)\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrnGY7_9UGUH",
        "outputId": "1f093b14-816e-4611-f074-2df3b4d1bd31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971ms/step - accuracy: 0.7500 - loss: 0.7075\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.7069\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.7061\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.7053\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.7046\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.7039\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.7032\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.7025\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.7018\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.7011\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.7004\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 0.6997\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.6990\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.6983\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.6976\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.6969\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6963\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7500 - loss: 0.6956\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6950\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 0.6943\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6937\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.6930\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6924\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6918\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7500 - loss: 0.6912\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.6905\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6899\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7500 - loss: 0.6894\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 0.6888\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7500 - loss: 0.6882\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6876\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7500 - loss: 0.6870\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7500 - loss: 0.6865\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6859\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7500 - loss: 0.6854\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7500 - loss: 0.6848\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7500 - loss: 0.6843\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6838\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6832\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.6827\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 0.6822\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6817\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7500 - loss: 0.6812\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6807\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7500 - loss: 0.6802\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6797\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6792\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.6788\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.6783\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6778\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6774\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6769\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.6765\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7500 - loss: 0.6760\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.6756\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7500 - loss: 0.6751\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.6747\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7500 - loss: 0.6743\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6738\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6734\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6730\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6726\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6722\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6718\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6714\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6712\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.6711\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6710\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.6708\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 0.6706\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7500 - loss: 0.6704\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6702\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6699\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.6696\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7500 - loss: 0.6694\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7500 - loss: 0.6691\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6688\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7500 - loss: 0.6684\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.6681\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7500 - loss: 0.6678\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6677\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6675\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6673\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.6671\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6669\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6666\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6664\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6661\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6659\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6656\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6654\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5000 - loss: 0.6652\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6650\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6648\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6646\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6644\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 0.6642\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6639\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.6637\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.6635\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.5000 - loss: 0.6633\n",
            "Loss: 0.663308322429657, Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How can you apply different activation functions in a neural network in Keras?"
      ],
      "metadata": {
        "id": "KfgW19dFS7zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LeakyReLU\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset (input features and target labels)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Example input (XOR problem)\n",
        "y = np.array([0, 1, 1, 0])  # Example output\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add input layer and hidden layer with ReLU activation\n",
        "model.add(Dense(units=8, input_dim=2, activation='relu'))  # ReLU in the hidden layer\n",
        "\n",
        "# Add another hidden layer with Leaky ReLU activation\n",
        "model.add(Dense(units=8, activation=LeakyReLU(alpha=0.1)))  # Leaky ReLU\n",
        "\n",
        "# Add output layer with Sigmoid activation (for binary classification)\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # Sigmoid for binary output\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=4)\n",
        "\n",
        "# Evaluate the model (optional)\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59IyP__4UWG0",
        "outputId": "3638eb6d-a3ea-4d52-994a-a1929f5059fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996ms/step - accuracy: 0.5000 - loss: 0.8040\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2500 - loss: 0.8019\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.7998\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2500 - loss: 0.7978\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2500 - loss: 0.7957\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7938\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.7918\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.7899\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2500 - loss: 0.7879\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 0.7860\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.7842\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2500 - loss: 0.7823\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.7805\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 0.7787\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2500 - loss: 0.7769\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.7752\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.7735\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7718\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7701\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.7684\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2500 - loss: 0.7668\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 0.7652\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 0.7635\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 0.7620\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2500 - loss: 0.7603\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2500 - loss: 0.7587\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2500 - loss: 0.7571\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2500 - loss: 0.7554\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7537\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7518\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 0.7500\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.7481\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2500 - loss: 0.7462\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2500 - loss: 0.7444\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2500 - loss: 0.7426\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.7408\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2500 - loss: 0.7390\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7372\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 0.7354\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7336\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.7318\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7301\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7283\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.7266\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2500 - loss: 0.7249\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2500 - loss: 0.7232\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 0.7215\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2500 - loss: 0.7198\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2500 - loss: 0.7181\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.2500 - loss: 0.7165\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.7153\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2500 - loss: 0.7141\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7129\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.7118\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.7106\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 0.7095\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.7083\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 0.7072\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 0.7061\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.7050\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.7039\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2500 - loss: 0.7028\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2500 - loss: 0.7018\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 0.7008\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2500 - loss: 0.7001\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2500 - loss: 0.6993\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2500 - loss: 0.6986\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.2500 - loss: 0.6978\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.2500 - loss: 0.6971\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2500 - loss: 0.6963\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2500 - loss: 0.6956\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2500 - loss: 0.6948\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 0.6941\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5000 - loss: 0.6933\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5000 - loss: 0.6926\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5000 - loss: 0.6919\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6911\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 0.6904\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.6897\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 0.6889\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.6882\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5000 - loss: 0.6875\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6868\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5000 - loss: 0.6861\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5000 - loss: 0.6853\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5000 - loss: 0.6846\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.5000 - loss: 0.6839\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.6832\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5000 - loss: 0.6824\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 0.6817\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5000 - loss: 0.6810\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 0.6803\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5000 - loss: 0.6796\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.6788\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.6781\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.6774\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6767\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5000 - loss: 0.6760\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7500 - loss: 0.6754\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7500 - loss: 0.6747\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.7500 - loss: 0.6741\n",
            "Loss: 0.6740630269050598, Accuracy: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "5. How do you add dropout to a neural network model to prevent overfitting?\n"
      ],
      "metadata": {
        "id": "4q_83Q75S-SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset (input features and target labels)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Example input (XOR problem)\n",
        "y = np.array([0, 1, 1, 0])  # Example output\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add input layer and first hidden layer\n",
        "model.add(Dense(units=8, input_dim=2, activation='relu'))\n",
        "\n",
        "# Add Dropout after the first hidden layer\n",
        "model.add(Dropout(0.3))  # 30% dropout\n",
        "\n",
        "# Add second hidden layer\n",
        "model.add(Dense(units=8, activation='relu'))\n",
        "\n",
        "# Add Dropout after the second hidden layer\n",
        "model.add(Dropout(0.3))  # 30% dropout\n",
        "\n",
        "# Add output layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # Sigmoid for binary output\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=4)\n",
        "\n",
        "# Evaluate the model (optional)\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zKAdW8FUyEf",
        "outputId": "dfde635c-a229-46b5-9f5d-32629761f885"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.7500 - loss: 0.7838\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6214\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.8300\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6544\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 0.8267\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 0.7879\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.5595\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2500 - loss: 0.7386\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6736\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6846\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5000 - loss: 0.8017\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5000 - loss: 0.6713\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.7185\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5000 - loss: 0.6896\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.7130\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6700\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.8987\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.8685\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7150\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.8861\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.9554\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.0000e+00 - loss: 0.9318\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 0.9017\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6618\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2500 - loss: 0.7867\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.5962\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6012\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 0.9493\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.6964\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.8450\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6871\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6546\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.2500 - loss: 0.7600\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5000 - loss: 0.7128\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2500 - loss: 0.9293\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.2500 - loss: 0.7466\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.6685\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2500 - loss: 0.7657\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6246\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.7726\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6174\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 0.5083\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6964\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7500 - loss: 0.6390\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.6478\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6904\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.7189\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2500 - loss: 0.8899\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7405\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.8212\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6820\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.8809\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.8031\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.8065\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6582\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.5533\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2500 - loss: 0.7201\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5000 - loss: 0.6936\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5000 - loss: 0.5861\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.6726\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.0000e+00 - loss: 0.8015\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.2500 - loss: 0.7930\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2500 - loss: 0.6755\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6645\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7971\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6598\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6515\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.8494\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.7935\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 0.7552\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7145\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.7572\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.7071\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.5690\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6966\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6142\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.6594\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 0.7229\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2500 - loss: 0.7596\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6884\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5000 - loss: 0.6955\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7500 - loss: 0.5657\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2500 - loss: 0.7209\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.2500 - loss: 0.7860\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.7370\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.8171\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6172\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.5938\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7500 - loss: 0.5420\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.5815\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 0.7900\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 0.8358\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2500 - loss: 0.8068\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.7776\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.7996\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6647\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.6407\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6714\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.7245\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 0.8821\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.2500 - loss: 0.6892\n",
            "Loss: 0.6892321109771729, Accuracy: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do you manually implement forward propagation in a simple neural network?\n"
      ],
      "metadata": {
        "id": "xHXpbUldTB8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example input data (XOR problem)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 4 samples, 2 features\n",
        "y = np.array([0, 1, 1, 0])  # XOR labels\n",
        "\n",
        "# Initialize random weights and biases\n",
        "# Hidden layer: 2 neurons, input has 2 features\n",
        "W1 = np.random.randn(2, 2)  # Weight matrix for input to hidden layer\n",
        "b1 = np.random.randn(2)  # Bias for hidden layer\n",
        "\n",
        "# Output layer: 1 neuron, hidden layer has 2 neurons\n",
        "W2 = np.random.randn(2, 1)  # Weight matrix for hidden to output layer\n",
        "b2 = np.random.randn(1)  # Bias for output layer\n",
        "\n",
        "# Activation function: ReLU for hidden layer and Sigmoid for output layer\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Forward propagation\n",
        "# 1. Compute hidden layer output\n",
        "Z1 = np.dot(X, W1) + b1  # Weighted sum for hidden layer\n",
        "A1 = relu(Z1)  # Apply ReLU activation\n",
        "\n",
        "# 2. Compute output layer output\n",
        "Z2 = np.dot(A1, W2) + b2  # Weighted sum for output layer\n",
        "A2 = sigmoid(Z2)  # Apply Sigmoid activation (final output)\n",
        "\n",
        "# Output of the neural network after forward propagation\n",
        "print(\"Output (Predictions):\")\n",
        "print(A2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J73Bh_0sU-Mm",
        "outputId": "8c350343-4519-4d33-a990-78346a000c36"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output (Predictions):\n",
            "[[0.69109641]\n",
            " [0.69905113]\n",
            " [0.23199201]\n",
            " [0.69109641]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you add batch normalization to a neural network model in Keras?"
      ],
      "metadata": {
        "id": "j9miDx9vTGGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset (XOR problem)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 4 samples, 2 features\n",
        "y = np.array([0, 1, 1, 0])  # XOR labels\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add input layer and hidden layer with Batch Normalization\n",
        "model.add(Dense(units=8, input_dim=2))  # Hidden layer with 8 neurons\n",
        "model.add(BatchNormalization())  # Apply Batch Normalization\n",
        "model.add(Dense(units=8))  # Another hidden layer\n",
        "model.add(BatchNormalization())  # Apply Batch Normalization\n",
        "\n",
        "# Add output layer with sigmoid activation for binary classification\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=4)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYTog637VLgC",
        "outputId": "61f4ff06-34ee-4861-9ef2-621cf5674dca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.7468\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.7404\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 0.7344\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5000 - loss: 0.7288\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.7237\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.7190\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.7148\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.7109\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5000 - loss: 0.7075\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5000 - loss: 0.7046\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.7020\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6999\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5000 - loss: 0.6982\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5000 - loss: 0.6969\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.6960\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5000 - loss: 0.6954\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6950\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 0.6948\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5000 - loss: 0.6947\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 0.6948\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.6949\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6949\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5000 - loss: 0.6950\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5000 - loss: 0.6950\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5000 - loss: 0.6949\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5000 - loss: 0.6948\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 0.6946\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5000 - loss: 0.6944\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5000 - loss: 0.6943\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5000 - loss: 0.6941\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.5000 - loss: 0.6940\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5000 - loss: 0.6939\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6939\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: 0.6938\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.6938\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6938\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6937\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6936\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6936\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6935\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6934\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6933\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6933\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.6933\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6933\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6933\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6933\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6933\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6933\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5000 - loss: 0.6933\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6931\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6932\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.6931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x7ed0109235b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.7500 - loss: 0.7299\n",
            "Loss: 0.7299352884292603, Accuracy: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "8. How can you visualize the training process with accuracy and loss curves?"
      ],
      "metadata": {
        "id": "O9Zek-TuTHfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Example dataset (XOR problem)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 4 samples, 2 features\n",
        "y = np.array([0, 1, 1, 0])  # XOR labels\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(units=8, input_dim=2, activation='relu'))\n",
        "model.add(Dense(units=8, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model and store the history\n",
        "history = model.fit(X, y, epochs=100, batch_size=4, validation_split=0.2)\n",
        "\n",
        "# Plot the accuracy and loss curves\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Show plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F62v5sCDVzUn",
        "outputId": "efd0a3bf-f1c2-4dae-a749-e720b3393cac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6667 - loss: 0.6554"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x7ed010aa1090> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.6667 - loss: 0.6554 - val_accuracy: 1.0000 - val_loss: 0.6836\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 0.6538 - val_accuracy: 1.0000 - val_loss: 0.6871\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 0.6521 - val_accuracy: 1.0000 - val_loss: 0.6906\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6667 - loss: 0.6504 - val_accuracy: 0.0000e+00 - val_loss: 0.6941\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6667 - loss: 0.6487 - val_accuracy: 0.0000e+00 - val_loss: 0.6975\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6667 - loss: 0.6471 - val_accuracy: 0.0000e+00 - val_loss: 0.7010\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 0.6454 - val_accuracy: 0.0000e+00 - val_loss: 0.7043\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6667 - loss: 0.6443 - val_accuracy: 0.0000e+00 - val_loss: 0.7074\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6667 - loss: 0.6432 - val_accuracy: 0.0000e+00 - val_loss: 0.7104\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6667 - loss: 0.6421 - val_accuracy: 0.0000e+00 - val_loss: 0.7134\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6667 - loss: 0.6410 - val_accuracy: 0.0000e+00 - val_loss: 0.7162\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6667 - loss: 0.6399 - val_accuracy: 0.0000e+00 - val_loss: 0.7190\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6667 - loss: 0.6388 - val_accuracy: 0.0000e+00 - val_loss: 0.7218\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6667 - loss: 0.6376 - val_accuracy: 0.0000e+00 - val_loss: 0.7245\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6667 - loss: 0.6365 - val_accuracy: 0.0000e+00 - val_loss: 0.7272\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.6667 - loss: 0.6354 - val_accuracy: 0.0000e+00 - val_loss: 0.7299\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.6667 - loss: 0.6343 - val_accuracy: 0.0000e+00 - val_loss: 0.7325\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6667 - loss: 0.6332 - val_accuracy: 0.0000e+00 - val_loss: 0.7352\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 0.6320 - val_accuracy: 0.0000e+00 - val_loss: 0.7378\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6667 - loss: 0.6309 - val_accuracy: 0.0000e+00 - val_loss: 0.7404\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6667 - loss: 0.6298 - val_accuracy: 0.0000e+00 - val_loss: 0.7430\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6667 - loss: 0.6288 - val_accuracy: 0.0000e+00 - val_loss: 0.7456\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6667 - loss: 0.6279 - val_accuracy: 0.0000e+00 - val_loss: 0.7482\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6667 - loss: 0.6269 - val_accuracy: 0.0000e+00 - val_loss: 0.7508\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6667 - loss: 0.6260 - val_accuracy: 0.0000e+00 - val_loss: 0.7533\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6667 - loss: 0.6250 - val_accuracy: 0.0000e+00 - val_loss: 0.7559\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6667 - loss: 0.6240 - val_accuracy: 0.0000e+00 - val_loss: 0.7585\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6667 - loss: 0.6231 - val_accuracy: 0.0000e+00 - val_loss: 0.7610\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6667 - loss: 0.6221 - val_accuracy: 0.0000e+00 - val_loss: 0.7636\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6667 - loss: 0.6212 - val_accuracy: 0.0000e+00 - val_loss: 0.7660\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6667 - loss: 0.6202 - val_accuracy: 0.0000e+00 - val_loss: 0.7684\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6667 - loss: 0.6193 - val_accuracy: 0.0000e+00 - val_loss: 0.7708\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.6667 - loss: 0.6184 - val_accuracy: 0.0000e+00 - val_loss: 0.7732\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6667 - loss: 0.6174 - val_accuracy: 0.0000e+00 - val_loss: 0.7757\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6667 - loss: 0.6165 - val_accuracy: 0.0000e+00 - val_loss: 0.7783\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6667 - loss: 0.6155 - val_accuracy: 0.0000e+00 - val_loss: 0.7808\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6667 - loss: 0.6146 - val_accuracy: 0.0000e+00 - val_loss: 0.7834\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6667 - loss: 0.6136 - val_accuracy: 0.0000e+00 - val_loss: 0.7859\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6667 - loss: 0.6127 - val_accuracy: 0.0000e+00 - val_loss: 0.7885\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6667 - loss: 0.6117 - val_accuracy: 0.0000e+00 - val_loss: 0.7911\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.6667 - loss: 0.6108 - val_accuracy: 0.0000e+00 - val_loss: 0.7937\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.6667 - loss: 0.6098 - val_accuracy: 0.0000e+00 - val_loss: 0.7962\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6667 - loss: 0.6089 - val_accuracy: 0.0000e+00 - val_loss: 0.7987\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6667 - loss: 0.6079 - val_accuracy: 0.0000e+00 - val_loss: 0.8013\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6667 - loss: 0.6070 - val_accuracy: 0.0000e+00 - val_loss: 0.8038\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6667 - loss: 0.6060 - val_accuracy: 0.0000e+00 - val_loss: 0.8064\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6667 - loss: 0.6051 - val_accuracy: 0.0000e+00 - val_loss: 0.8091\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6667 - loss: 0.6041 - val_accuracy: 0.0000e+00 - val_loss: 0.8118\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6667 - loss: 0.6032 - val_accuracy: 0.0000e+00 - val_loss: 0.8143\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 0.6022 - val_accuracy: 0.0000e+00 - val_loss: 0.8166\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.6667 - loss: 0.6012 - val_accuracy: 0.0000e+00 - val_loss: 0.8190\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.6667 - loss: 0.6002 - val_accuracy: 0.0000e+00 - val_loss: 0.8214\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.6667 - loss: 0.5993 - val_accuracy: 0.0000e+00 - val_loss: 0.8239\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.6667 - loss: 0.5983 - val_accuracy: 0.0000e+00 - val_loss: 0.8265\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6667 - loss: 0.5973 - val_accuracy: 0.0000e+00 - val_loss: 0.8290\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.6667 - loss: 0.5963 - val_accuracy: 0.0000e+00 - val_loss: 0.8316\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.6667 - loss: 0.5953 - val_accuracy: 0.0000e+00 - val_loss: 0.8343\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.6667 - loss: 0.5943 - val_accuracy: 0.0000e+00 - val_loss: 0.8369\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.6667 - loss: 0.5933 - val_accuracy: 0.0000e+00 - val_loss: 0.8395\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.6667 - loss: 0.5923 - val_accuracy: 0.0000e+00 - val_loss: 0.8421\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.6667 - loss: 0.5913 - val_accuracy: 0.0000e+00 - val_loss: 0.8447\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.6667 - loss: 0.5903 - val_accuracy: 0.0000e+00 - val_loss: 0.8475\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.6667 - loss: 0.5893 - val_accuracy: 0.0000e+00 - val_loss: 0.8501\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.6667 - loss: 0.5883 - val_accuracy: 0.0000e+00 - val_loss: 0.8527\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.6667 - loss: 0.5873 - val_accuracy: 0.0000e+00 - val_loss: 0.8554\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6667 - loss: 0.5863 - val_accuracy: 0.0000e+00 - val_loss: 0.8581\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.6667 - loss: 0.5853 - val_accuracy: 0.0000e+00 - val_loss: 0.8609\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.6667 - loss: 0.5843 - val_accuracy: 0.0000e+00 - val_loss: 0.8637\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.6667 - loss: 0.5833 - val_accuracy: 0.0000e+00 - val_loss: 0.8665\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.6667 - loss: 0.5823 - val_accuracy: 0.0000e+00 - val_loss: 0.8694\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.6667 - loss: 0.5813 - val_accuracy: 0.0000e+00 - val_loss: 0.8721\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.6667 - loss: 0.5803 - val_accuracy: 0.0000e+00 - val_loss: 0.8748\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6667 - loss: 0.5792 - val_accuracy: 0.0000e+00 - val_loss: 0.8776\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6667 - loss: 0.5782 - val_accuracy: 0.0000e+00 - val_loss: 0.8805\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 0.5772 - val_accuracy: 0.0000e+00 - val_loss: 0.8834\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6667 - loss: 0.5762 - val_accuracy: 0.0000e+00 - val_loss: 0.8864\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 0.5752 - val_accuracy: 0.0000e+00 - val_loss: 0.8893\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6667 - loss: 0.5742 - val_accuracy: 0.0000e+00 - val_loss: 0.8923\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6667 - loss: 0.5732 - val_accuracy: 0.0000e+00 - val_loss: 0.8953\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6667 - loss: 0.5721 - val_accuracy: 0.0000e+00 - val_loss: 0.8982\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6667 - loss: 0.5711 - val_accuracy: 0.0000e+00 - val_loss: 0.9012\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.6667 - loss: 0.5701 - val_accuracy: 0.0000e+00 - val_loss: 0.9044\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6667 - loss: 0.5691 - val_accuracy: 0.0000e+00 - val_loss: 0.9075\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6667 - loss: 0.5682 - val_accuracy: 0.0000e+00 - val_loss: 0.9106\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6667 - loss: 0.5672 - val_accuracy: 0.0000e+00 - val_loss: 0.9137\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6667 - loss: 0.5662 - val_accuracy: 0.0000e+00 - val_loss: 0.9170\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6667 - loss: 0.5653 - val_accuracy: 0.0000e+00 - val_loss: 0.9203\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6667 - loss: 0.5644 - val_accuracy: 0.0000e+00 - val_loss: 0.9236\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 0.5634 - val_accuracy: 0.0000e+00 - val_loss: 0.9269\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6667 - loss: 0.5625 - val_accuracy: 0.0000e+00 - val_loss: 0.9300\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6667 - loss: 0.5616 - val_accuracy: 0.0000e+00 - val_loss: 0.9331\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6667 - loss: 0.5606 - val_accuracy: 0.0000e+00 - val_loss: 0.9363\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6667 - loss: 0.5597 - val_accuracy: 0.0000e+00 - val_loss: 0.9396\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 0.5588 - val_accuracy: 0.0000e+00 - val_loss: 0.9428\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6667 - loss: 0.5579 - val_accuracy: 0.0000e+00 - val_loss: 0.9458\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.6667 - loss: 0.5570 - val_accuracy: 0.0000e+00 - val_loss: 0.9489\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6667 - loss: 0.5561 - val_accuracy: 0.0000e+00 - val_loss: 0.9520\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 0.5552 - val_accuracy: 0.0000e+00 - val_loss: 0.9551\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6667 - loss: 0.5543 - val_accuracy: 0.0000e+00 - val_loss: 0.9584\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6667 - loss: 0.5535 - val_accuracy: 0.0000e+00 - val_loss: 0.9617\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "9. How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients?"
      ],
      "metadata": {
        "id": "Ah6SHjyXTIco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset (XOR problem)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 4 samples, 2 features\n",
        "y = np.array([0, 1, 1, 0])  # XOR labels\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(units=8, input_dim=2, activation='relu'))\n",
        "model.add(Dense(units=8, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model with gradient clipping by value\n",
        "optimizer = Adam(clipvalue=1.0)  # Clipping gradient values at 1.0\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8jJP-RcWCop",
        "outputId": "c55c89e8-9ad5-41b8-914d-289377d03dee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.7500 - loss: 0.6630\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6621\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.6613\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6605\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 0.6597\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7500 - loss: 0.6588\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6580\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 0.6571\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.6563\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7500 - loss: 0.6555\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7500 - loss: 0.6546\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.6537\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.6529\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.6521\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.6513\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6504\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.6496\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6488\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6480\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7500 - loss: 0.6472\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7500 - loss: 0.6464\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7500 - loss: 0.6455\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.6447\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7500 - loss: 0.6439\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7500 - loss: 0.6431\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7500 - loss: 0.6423\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.6414\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.6406\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7500 - loss: 0.6398\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6389\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6381\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7500 - loss: 0.6372\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.6364\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7500 - loss: 0.6355\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7500 - loss: 0.6347\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 0.6338\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7500 - loss: 0.6330\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.6321\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.6312\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7500 - loss: 0.6305\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.6298\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.6291\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.6283\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.6276\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.6269\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.6261\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.6253\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.6245\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.6237\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.6229\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.6221\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.6212\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.6204\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.6196\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.6187\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.6179\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.6170\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6163\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.6156\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.6149\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6142\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.6135\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.6128\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.6120\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.6111\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.6103\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.6095\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.6087\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.6078\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.6071\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.6064\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.6056\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.6048\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.6040\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.6032\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.6024\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.6015\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 0.6008\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.6000\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.5992\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.5984\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.5976\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.5968\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.5960\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.5951\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.5944\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.5936\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.5928\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 1.0000 - loss: 0.5919\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 1.0000 - loss: 0.5910\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.5903\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.5894\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.5886\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.5877\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.5868\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.5860\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.5851\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.5842\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.5834\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.5826\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ed001c3d6c0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "10. How can you create a custom loss function in Keras?"
      ],
      "metadata": {
        "id": "mMQhY0X0TKvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Define the custom loss function: Mean Absolute Percentage Error (MAPE)\n",
        "def custom_mape(y_true, y_pred):\n",
        "    # Clip the predictions to avoid division by zero\n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
        "\n",
        "    # Calculate MAPE\n",
        "    mape = tf.reduce_mean(tf.abs((y_true - y_pred) / y_true)) * 100\n",
        "    return mape\n",
        "\n",
        "# Example dataset (XOR problem)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 4 samples, 2 features\n",
        "y = np.array([0, 1, 1, 0])  # XOR labels\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(units=8, input_dim=2, activation='relu'))\n",
        "model.add(Dense(units=8, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model with the custom loss function\n",
        "model.compile(optimizer=Adam(), loss=custom_mape, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WMRV_NrWgOC",
        "outputId": "f1836010-de24-4893-c3e3-67fd38135518"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.7500 - loss: inf\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: nan\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: nan\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ed001c470d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "11. How can you visualize the structure of a neural network model in Keras?"
      ],
      "metadata": {
        "id": "I2qoNKehTMAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Define a simple model\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=2, activation='relu', name='input_layer'))\n",
        "model.add(Dense(64, activation='relu', name='hidden_layer'))\n",
        "model.add(Dense(1, activation='sigmoid', name='output_layer'))\n",
        "\n",
        "# Print the summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "4L7cXHS4WpLO",
        "outputId": "2c6605c2-d109-477a-ac93-19cb001be7c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │              \u001b[38;5;34m96\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ hidden_layer (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m2,112\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ output_layer (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ hidden_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,273\u001b[0m (8.88 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,273</span> (8.88 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,273\u001b[0m (8.88 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,273</span> (8.88 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}