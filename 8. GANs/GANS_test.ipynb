{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**GANS**"
      ],
      "metadata": {
        "id": "9fO8d8sgxa_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What does GAN stand for, and what is its main purpose?\n"
      ],
      "metadata": {
        "id": "B4DRa2JbxdZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN stands for **Generative Adversarial Network**. It's a type of artificial intelligence algorithm used for generating data that is similar to a given dataset. GANs are composed of two parts: a generator and a discriminator.\n",
        "\n",
        "- The **generator** creates new data instances that resemble the training data.\n",
        "- The **discriminator** evaluates the data and tries to determine if it is real (from the training data) or fake (created by the generator).\n",
        "\n",
        "The main purpose of GANs is to generate realistic data that can be used in various applications, such as creating images, music, and even realistic video game environments. They are especially known for their ability to produce highly realistic and high-quality outputs."
      ],
      "metadata": {
        "id": "RXLyvsoGxr1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concept of the \"discriminator\" in GANS.\n"
      ],
      "metadata": {
        "id": "kFp908Atx2t9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **discriminator** in a Generative Adversarial Network (GAN) is like a vigilant detective. It has a vital role in the GAN's framework, which consists of two neural networks—the generator and the discriminator—locked in a continuous game of cat and mouse.\n",
        "\n",
        "The main purpose of the discriminator is to distinguish between real data (from the training set) and fake data (generated by the generator). Here’s how it works:\n",
        "\n",
        "1. **Training Data**: The discriminator is fed real data from the dataset and fake data generated by the generator.\n",
        "2. **Binary Classification**: It acts as a binary classifier, labeling each piece of data as real (1) or fake (0).\n",
        "3. **Feedback Loop**: It provides feedback to the generator based on its evaluations, helping the generator improve its data creation over time.\n",
        "\n",
        "The discriminator’s job is to become incredibly adept at telling real from fake. Simultaneously, the generator learns to create more convincing fake data to fool the discriminator. This adversarial training process continues until the generator produces data that is so realistic, the discriminator can no longer tell the difference.\n",
        "\n",
        "In essence, the discriminator ensures that the generator continuously improves, resulting in highly realistic synthetic data."
      ],
      "metadata": {
        "id": "QZ5pIBp8x4Xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. How does a GAN work?\n"
      ],
      "metadata": {
        "id": "qW0iIJ217x6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Generative Adversarial Network (GAN) operates through a clever, iterative process involving two neural networks—the generator and the discriminator—engaged in an adversarial game. Here’s a simplified breakdown of how a GAN works:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - Both the generator and the discriminator are initialized with random weights.\n",
        "\n",
        "2. **Generating Data**:\n",
        "   - The generator takes in random noise (a vector of random numbers) and transforms it into a data sample (e.g., an image).\n",
        "\n",
        "3. **Discriminating**:\n",
        "   - The discriminator evaluates both real data from the training dataset and the synthetic data created by the generator. It aims to classify each piece of data as real or fake.\n",
        "\n",
        "4. **Loss Calculation and Feedback**:\n",
        "   - The discriminator calculates a loss based on its classification accuracy.\n",
        "   - The generator calculates its own loss based on the discriminator's feedback on its fake data.\n",
        "\n",
        "5. **Updating Weights**:\n",
        "   - The discriminator's weights are updated to better distinguish between real and fake data.\n",
        "   - The generator’s weights are updated to produce more convincing fake data, based on the discriminator’s feedback.\n",
        "\n",
        "6. **Adversarial Training Loop**:\n",
        "   - The generator and discriminator continue this process in a loop.\n",
        "   - Over time, the generator gets better at creating realistic data, and the discriminator gets better at detecting fake data.\n",
        "   - The aim is to reach a point where the discriminator can no longer distinguish between real and generated data.\n",
        "\n",
        "**Illustration**:\n",
        "\n",
        "```\n",
        "Generator -> Fake Data -> Discriminator\n",
        "              |                  |\n",
        "          Random Noise           |\n",
        "              |                  |\n",
        "Training Data -> Real Data ------|\n",
        "```\n",
        "\n",
        "The interplay between the generator and the discriminator drives the improvement of both networks. This adversarial process leads to the generator creating highly realistic data samples, which can be images, audio, or other types of data, depending on the application. It’s quite a dance of creativity and scrutiny, fostering innovation in synthetic data generation!"
      ],
      "metadata": {
        "id": "4Us_qxzu726e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the generator's role in a GAN?\n"
      ],
      "metadata": {
        "id": "POqp4mwKCbUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **generator** in a Generative Adversarial Network (GAN) has a creative and challenging role. Its main task is to produce data that is as realistic as possible, tricking the discriminator into believing it’s authentic. Here’s how the generator contributes to the GAN:\n",
        "\n",
        "1. **Generating Data**: The generator starts by taking a random input, usually a vector of random noise, and processes it through a series of layers to generate data. This data could be images, audio, or other types of data, depending on the specific application of the GAN.\n",
        "\n",
        "2. **Improvement Through Feedback**: The generator's output is evaluated by the discriminator, which provides feedback on how realistic the generated data is. Initially, the generated data may not be very convincing, but as the training progresses, the generator uses the feedback to improve and create more realistic data.\n",
        "\n",
        "3. **Adversarial Role**: The generator constantly tries to fool the discriminator by improving its data generation capabilities. As the discriminator becomes better at identifying fake data, the generator also evolves, creating a continuous cycle of improvement for both networks.\n",
        "\n",
        "4. **Learning Process**: The generator’s learning process is driven by the discriminator's feedback, which is used to update the generator’s parameters. This helps it learn the patterns and features of the real data, allowing it to produce more convincing fakes over time.\n",
        "\n",
        "In essence, the generator is the imaginative artist, continually refining its craft to create realistic and high-quality data, pushing the boundaries of what it can produce. It’s an intriguing dynamic of creativity and competition, leading to remarkable advancements in synthetic data generation!"
      ],
      "metadata": {
        "id": "TxyA3pM8CdlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is the loss function used in the training of GANS?\n"
      ],
      "metadata": {
        "id": "lERxxrjIDOHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training of Generative Adversarial Networks (GANs) involves two distinct loss functions—one for the generator and one for the discriminator. These loss functions are crucial as they guide the networks in improving their performance through backpropagation.\n",
        "\n",
        "### **Discriminator Loss**\n",
        "The discriminator aims to maximize its ability to differentiate between real and fake data. The discriminator loss measures how well it can classify real data as real and generated data as fake. It is defined as follows:\n",
        "\\[\n",
        "\\text{Loss}_D = -\\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] - \\mathbb{E}_{z \\sim p_z}[\\log (1 - D(G(z)))]\n",
        "\\]\n",
        "- \\( x \\sim p_{\\text{data}} \\): Real data sampled from the data distribution.\n",
        "- \\( z \\sim p_z \\): Random noise sampled from a prior distribution (often Gaussian).\n",
        "- \\( D(x) \\): Discriminator’s estimate of the probability that real data \\( x \\) is real.\n",
        "- \\( D(G(z)) \\): Discriminator’s estimate of the probability that generated data \\( G(z) \\) is real.\n",
        "\n",
        "### **Generator Loss**\n",
        "The generator's goal is to fool the discriminator into classifying the generated data as real. The generator loss measures how effectively the generator can create realistic data. It is defined as:\n",
        "\\[\n",
        "\\text{Loss}_G = -\\mathbb{E}_{z \\sim p_z}[\\log D(G(z))]\n",
        "\\]\n",
        "- \\( z \\sim p_z \\): Random noise sampled from a prior distribution.\n",
        "- \\( D(G(z)) \\): Discriminator’s estimate of the probability that generated data \\( G(z) \\) is real.\n",
        "\n",
        "### **Objective**\n",
        "The overall training objective of GANs can be thought of as a minimax game between the generator and discriminator:\n",
        "\\[\n",
        "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log (1 - D(G(z)))]\n",
        "\\]\n",
        "\n",
        "### **Adversarial Training**\n",
        "During training:\n",
        "1. The discriminator is trained to maximize its loss function, improving its ability to distinguish real data from fake data.\n",
        "2. The generator is trained to minimize its loss function, improving its ability to generate realistic data that can fool the discriminator.\n",
        "\n",
        "By iteratively updating both the generator and discriminator, GANs improve their performance, resulting in the generator producing highly realistic data over time."
      ],
      "metadata": {
        "id": "1bmxLpNTDPgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the difference between a WGAN and a traditional GAN?\n"
      ],
      "metadata": {
        "id": "orSP8pwhEUGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WGAN** (Wasserstein GAN) and traditional **GAN** (Generative Adversarial Network) both aim to generate realistic data through adversarial training, but they differ in their training methodology and the underlying principles:\n",
        "\n",
        "### **Key Differences:**\n",
        "\n",
        "1. **Loss Function**:\n",
        "   - **Traditional GAN**: Uses a binary cross-entropy loss. This can lead to issues like vanishing gradients, making it difficult for the generator to improve.\n",
        "   - **WGAN**: Uses the Wasserstein distance (or Earth Mover's Distance) as the loss function, which provides more stable gradients, leading to more reliable training.\n",
        "\n",
        "2. **Training Stability**:\n",
        "   - **Traditional GAN**: Can suffer from unstable training and mode collapse, where the generator produces limited diversity in the output.\n",
        "   - **WGAN**: Offers improved training stability and reduces the risk of mode collapse by providing smoother gradients.\n",
        "\n",
        "3. **Discriminator vs. Critic**:\n",
        "   - **Traditional GAN**: Uses a discriminator that classifies inputs as real or fake.\n",
        "   - **WGAN**: Replaces the discriminator with a critic that scores the realness of the inputs, rather than classifying them.\n",
        "\n",
        "4. **Gradient Clipping**:\n",
        "   - **Traditional GAN**: Does not inherently use gradient clipping.\n",
        "   - **WGAN**: Enforces weight clipping to ensure the critic’s lipschitz constraint, which is necessary for calculating the Wasserstein distance.\n",
        "\n",
        "5. **Training Dynamics**:\n",
        "   - **Traditional GAN**: The discriminator aims to maximize its accuracy in differentiating real from fake, and the generator minimizes this accuracy.\n",
        "   - **WGAN**: The critic aims to estimate the Wasserstein distance between real and generated data distributions, and the generator minimizes this distance.\n",
        "\n",
        "### **Illustration**:\n",
        "\n",
        "| Aspect                  | Traditional GAN                               | WGAN                                       |\n",
        "|-------------------------|-----------------------------------------------|--------------------------------------------|\n",
        "| Loss Function           | Binary cross-entropy loss                     | Wasserstein distance                       |\n",
        "| Stability               | Prone to instability and mode collapse        | More stable and less prone to mode collapse|\n",
        "| Discriminator/Critic    | Classifies data as real or fake               | Scores data based on realness              |\n",
        "| Gradient Clipping       | Not typically used                            | Uses weight clipping                       |\n",
        "| Training Objective      | Minimize/maximize accuracy of real vs fake    | Minimize Wasserstein distance              |\n",
        "\n",
        "### **Impact**:\n",
        "WGANs have made significant strides in addressing some of the challenges faced by traditional GANs, particularly in terms of stability and quality of generated data. By using the Wasserstein distance, WGANs provide a more robust framework for adversarial training, leading to better and more reliable outcomes in data generation tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "Reenh-weEWrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How does the training of the generator differ from that of the discriminator?\n"
      ],
      "metadata": {
        "id": "YFid6g8LEkYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training of the generator and the discriminator in a Generative Adversarial Network (GAN) involves different objectives and methodologies. Here’s a breakdown of how their training processes differ:\n",
        "\n",
        "### **Discriminator Training**\n",
        "**Objective**: The discriminator aims to differentiate between real data and fake data produced by the generator.\n",
        "\n",
        "1. **Real vs. Fake**: It is trained on both real data from the training dataset and fake data generated by the generator.\n",
        "2. **Loss Function**: The discriminator uses a loss function like binary cross-entropy to measure how well it distinguishes real data from fake data.\n",
        "3. **Updates**: The discriminator’s weights are updated to improve its classification accuracy, becoming better at identifying fake data over time.\n",
        "\n",
        "### **Generator Training**\n",
        "**Objective**: The generator aims to produce data that is indistinguishable from real data, fooling the discriminator.\n",
        "\n",
        "1. **Generating Data**: The generator takes random noise as input and transforms it into data (e.g., an image).\n",
        "2. **Feedback Loop**: It relies on feedback from the discriminator to understand how realistic its generated data is.\n",
        "3. **Loss Function**: The generator's loss function measures how well it can fool the discriminator. In a traditional GAN, this might be the negative log probability that the discriminator assigns to fake data.\n",
        "4. **Updates**: The generator’s weights are updated to create more realistic data, making it harder for the discriminator to tell the difference between real and fake data.\n",
        "\n",
        "### **Training Dynamics**\n",
        "- **Adversarial Relationship**: The discriminator and generator are trained simultaneously but adversarially. The discriminator tries to improve its ability to detect fake data, while the generator improves its ability to produce realistic data.\n",
        "- **Iteration**: The training process involves alternating updates to the discriminator and generator. Typically, for each batch of data, the discriminator is updated first, followed by the generator.\n",
        "\n",
        "### **Illustration**:\n",
        "\n",
        "| **Aspect**         | **Discriminator**                          | **Generator**                        |\n",
        "|--------------------|--------------------------------------------|--------------------------------------|\n",
        "| **Objective**      | Distinguish real data from fake data       | Generate realistic data              |\n",
        "| **Input Data**     | Real data + Fake data                      | Random noise                         |\n",
        "| **Loss Function**  | Binary cross-entropy (or similar)          | Negative log probability (or similar)|\n",
        "| **Training Goal**  | Improve classification accuracy            | Fool the discriminator               |\n",
        "| **Updates**        | Weights updated to enhance classification  | Weights updated to enhance realism   |\n",
        "\n",
        "By training these two networks in tandem, GANs achieve a balanced improvement where the generator continuously learns to create more convincing data, and the discriminator learns to better identify real versus generated data. This adversarial training process is what enables GANs to generate highly realistic synthetic data."
      ],
      "metadata": {
        "id": "Vsv4J4jIEmBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is a DCGAN, and how is it different from a traditional GAN?\n"
      ],
      "metadata": {
        "id": "evdWoP67Eqkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **DCGAN** (Deep Convolutional Generative Adversarial Network) is a type of GAN that uses deep convolutional neural networks (CNNs) to improve the stability and quality of the generated data, typically images. Here’s how DCGANs differ from traditional GANs:\n",
        "\n",
        "### **Key Features of DCGANs**:\n",
        "\n",
        "1. **Convolutional Layers**:\n",
        "   - **DCGAN**: Utilizes convolutional layers in both the generator and discriminator, which are well-suited for image data as they can capture spatial hierarchies and features efficiently.\n",
        "   - **Traditional GAN**: May use fully connected layers, which are less effective at capturing image data's spatial structure.\n",
        "\n",
        "2. **Architectural Enhancements**:\n",
        "   - **DCGAN**: Implements several architectural improvements, such as:\n",
        "     - **Strided Convolutions**: Instead of pooling layers, which helps in learning the spatial hierarchy.\n",
        "     - **Batch Normalization**: Stabilizes learning by normalizing inputs to each layer, reducing internal covariate shift.\n",
        "     - **ReLU and Tanh Activations**: Uses ReLU activations in the generator (except the output layer, which uses Tanh) to speed up convergence and improve training dynamics.\n",
        "   - **Traditional GAN**: Lacks these specific architectural enhancements, often resulting in less stable training and lower-quality outputs.\n",
        "\n",
        "3. **Improved Training Stability**:\n",
        "   - **DCGAN**: The use of convolutional layers and batch normalization contributes to more stable and reliable training, enabling the generation of higher-quality images.\n",
        "   - **Traditional GAN**: May suffer from issues like mode collapse and instability, resulting in poorer quality data.\n",
        "\n",
        "### **Illustration of DCGAN Architecture**:\n",
        "\n",
        "#### **Generator**:\n",
        "- Uses transposed convolutional layers (also known as deconvolutional layers) to generate images from random noise.\n",
        "- Activation functions: ReLU in hidden layers, Tanh in the output layer.\n",
        "\n",
        "#### **Discriminator**:\n",
        "- Uses convolutional layers to classify images as real or fake.\n",
        "- Activation functions: Leaky ReLU in hidden layers, sigmoid in the output layer.\n",
        "\n",
        "### **Comparison Table**:\n",
        "\n",
        "| Aspect                   | Traditional GAN                           | DCGAN                                              |\n",
        "|--------------------------|-------------------------------------------|----------------------------------------------------|\n",
        "| **Layer Types**          | Fully connected layers                    | Convolutional layers                               |\n",
        "| **Pooling**              | Max/average pooling                       | Strided convolutions                               |\n",
        "| **Normalization**        | None                                       | Batch normalization                                |\n",
        "| **Activation Functions** | Sigmoid/tanh throughout                   | ReLU in generator (hidden layers), Tanh (output)   |\n",
        "| **Training Stability**   | Can be unstable                           | More stable due to architectural enhancements      |\n",
        "| **Output Quality**       | Lower quality images                      | Higher quality images due to better feature capture|\n",
        "\n",
        "### **Impact**:\n",
        "DCGANs represent a significant improvement over traditional GANs, particularly for tasks involving image generation. The use of deep convolutional layers allows DCGANs to capture intricate features and produce more detailed and realistic images, making them a popular choice in various computer vision applications.\n",
        "\n"
      ],
      "metadata": {
        "id": "ErWAO_r1Evco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the concept of \"controllable generation\" in the context of GANS\n"
      ],
      "metadata": {
        "id": "z71MvxPSExOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Controllable generation** in the context of Generative Adversarial Networks (GANs) refers to the ability to guide or influence the GAN's output to meet specific criteria or constraints. This allows users to generate data with desired characteristics or properties. Here’s how controllable generation works and its significance:\n",
        "\n",
        "### **Core Concepts**:\n",
        "\n",
        "1. **Latent Space Manipulation**:\n",
        "   - GANs operate by mapping a random noise vector (latent code) to an output space (e.g., images). By manipulating this latent space, one can control the attributes of the generated output.\n",
        "   - For example, adjusting certain dimensions of the latent vector might change the color, shape, or style of the generated images.\n",
        "\n",
        "2. **Conditional GANs (cGANs)**:\n",
        "   - Conditional GANs extend traditional GANs by conditioning the generation process on additional information, such as class labels or other auxiliary data.\n",
        "   - This allows for the generation of specific categories of data, like generating images of cats versus dogs, or controlling attributes like age, gender, or expression in face generation.\n",
        "\n",
        "3. **Feature Embedding**:\n",
        "   - Features of the input data can be embedded into the latent space, enabling more granular control over the generated data. For instance, in text-to-image GANs, textual descriptions can be embedded to guide the generation process.\n",
        "\n",
        "4. **Interpretable Latent Representations**:\n",
        "   - Researchers work on making the latent space more interpretable, so that specific attributes in the output can be controlled by modifying corresponding latent codes. This is crucial for applications where precise control over the generated data is necessary.\n",
        "\n",
        "### **Applications**:\n",
        "\n",
        "- **Art and Design**: Artists can use controllable GANs to generate artwork with specific styles or elements, enhancing creativity and productivity.\n",
        "- **Fashion**: Designers can create new clothing designs by adjusting attributes such as color, pattern, and style.\n",
        "- **Face Generation**: Controllable GANs can generate faces with specific attributes like age, gender, or expression, useful in entertainment and security.\n",
        "- **Text-to-Image Synthesis**: Generating images that match specific textual descriptions, useful in advertising and media creation.\n",
        "\n",
        "### **Example**:\n",
        "Imagine you want to generate images of cars with specific features, such as color, type, or model. By using a controllable GAN, you can input these desired features, and the GAN will generate images that match those specifications. This approach is much more efficient than manually adjusting outputs or generating large datasets from scratch.\n",
        "\n",
        "### **Benefits**:\n",
        "\n",
        "- **Customization**: Allows for tailored generation based on user preferences or requirements.\n",
        "- **Efficiency**: Reduces the need for extensive datasets, as specific data points can be generated on demand.\n",
        "- **Creativity**: Enhances creative processes by providing new tools for artists, designers, and other creative professionals.\n",
        "\n",
        "Controllable generation in GANs represents a significant advancement, enabling more precise and useful applications in various fields. It turns the generation process from a random and often unpredictable one into a more guided and intentional creation, unlocking new potentials for AI-driven innovation."
      ],
      "metadata": {
        "id": "7Yo-pnMIEx7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the primary goal of training a GAN?\n"
      ],
      "metadata": {
        "id": "6cMCRnz8E2Gz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary goal of training a Generative Adversarial Network (GAN) is to create a generator that produces data indistinguishable from real data. This involves two neural networks—the generator and the discriminator—engaged in a competitive process. Here’s how it breaks down:\n",
        "\n",
        "### **Goals of Each Network**:\n",
        "\n",
        "1. **Generator**:\n",
        "   - **Objective**: Generate realistic data (e.g., images, text) that can fool the discriminator into thinking it is real.\n",
        "   - **Training**: The generator is trained to minimize its loss function, which reflects how convincingly it can produce data that the discriminator believes is real.\n",
        "\n",
        "2. **Discriminator**:\n",
        "   - **Objective**: Accurately distinguish between real data (from the training set) and fake data (produced by the generator).\n",
        "   - **Training**: The discriminator is trained to maximize its ability to correctly classify real vs. fake data, thereby improving its classification accuracy.\n",
        "\n",
        "### **Overall Training Objective**:\n",
        "The primary goal is to achieve an equilibrium where the discriminator can no longer distinguish between real and generated data. This is accomplished through the following adversarial training process:\n",
        "\n",
        "1. **Initial Phase**: The generator starts with random noise to create fake data, which is usually not very realistic.\n",
        "2. **Discrimination**: The discriminator assesses both real data and the fake data, providing feedback on its authenticity.\n",
        "3. **Improvement Cycle**:\n",
        "   - The generator uses this feedback to improve its data generation, making it more realistic over time.\n",
        "   - The discriminator also improves its ability to distinguish real from fake as it encounters more diverse and convincing fake data.\n",
        "4. **Convergence**: The process continues until the generator produces data so realistic that the discriminator’s performance in distinguishing real from fake reaches its limit. This signifies the generator has learned to generate highly realistic data.\n",
        "\n",
        "### **Ultimate Goal**:\n",
        "The ultimate goal is for the generator to create outputs that are virtually indistinguishable from real data to human observers, thereby pushing the boundaries of synthetic data generation.\n",
        "\n",
        "This adversarial dynamic not only propels both networks toward continual improvement but also enables GANs to achieve remarkable results in fields like image and video generation, data augmentation, and beyond. Isn’t it fascinating how this continuous feedback loop drives such impressive innovation?"
      ],
      "metadata": {
        "id": "k4TA8i8YE5B7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are the limitations of GANs?\n"
      ],
      "metadata": {
        "id": "idIBCEwBE_FE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While Generative Adversarial Networks (GANs) have revolutionized the field of data generation, they do come with several limitations:\n",
        "\n",
        "### **1. Training Instability**:\n",
        "GANs are notorious for unstable training dynamics. It can be challenging to balance the training of the generator and discriminator, often resulting in issues such as:\n",
        "- **Non-convergence**: The networks may not reach a stable equilibrium.\n",
        "- **Mode Collapse**: The generator produces a limited variety of outputs, ignoring parts of the data distribution.\n",
        "\n",
        "### **2. Require Extensive Data**:\n",
        "GANs need large and diverse datasets for effective training. They struggle to generalize well with small or biased datasets, limiting their applicability in data-scarce scenarios.\n",
        "\n",
        "### **3. Computationally Intensive**:\n",
        "Training GANs requires significant computational resources, including high-powered GPUs and extensive memory. This makes them less accessible for researchers and developers with limited resources.\n",
        "\n",
        "### **4. Evaluation Challenges**:\n",
        "Measuring the quality of generated data is complex and often subjective. There is no universally accepted metric for evaluating GAN performance, making comparisons and improvements difficult.\n",
        "\n",
        "### **5. Sensitivity to Hyperparameters**:\n",
        "GANs are highly sensitive to the choice of hyperparameters, such as learning rates, batch sizes, and network architectures. Finding the optimal settings often requires extensive experimentation and tuning.\n",
        "\n",
        "### **6. Practical Limitations**:\n",
        "- **Bias in Training Data**: If the training data contains biases, the GAN will learn and replicate these biases, potentially leading to ethical concerns.\n",
        "- **Application-Specific Challenges**: Different applications of GANs, like text generation or high-resolution image synthesis, present unique challenges that are not yet fully addressed by current models.\n",
        "\n",
        "### **7. Adversarial Nature**:\n",
        "The adversarial training process can be inherently competitive and antagonistic, sometimes leading to overfitting or underfitting, where the generator or discriminator becomes too good at its task, disrupting the balance.\n",
        "\n",
        "Despite these limitations, ongoing research and advancements continue to improve GANs, making them more stable, efficient, and versatile. The journey of mastering GANs is still unfolding, and each innovation brings us closer to overcoming these challenges."
      ],
      "metadata": {
        "id": "nwpxm-0SFAiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What are StyleGANS, and what makes them unique?\n"
      ],
      "metadata": {
        "id": "S03Gr1COFJLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StyleGAN** (Style Generative Adversarial Network) is an advanced type of GAN introduced by Nvidia researchers in December 2018. It stands out due to its unique architecture and capabilities:\n",
        "\n",
        "### **Key Features of StyleGAN**:\n",
        "\n",
        "1. **Style-Based Generator**:\n",
        "   - **StyleGAN** uses a style-based generator architecture, which allows for fine-grained control over the visual attributes of generated images. This means you can manipulate specific features like hairstyle, facial expression, or background details independently.\n",
        "\n",
        "2. **Intermediate Latent Space (W Space)**:\n",
        "   - Unlike traditional GANs that directly map a noise vector to generated data, StyleGAN introduces an intermediate latent space (W space) between the input noise vector (Z space) and the generator. This allows for more intuitive and disentangled control over the generated images.\n",
        "\n",
        "3. **Progressive Growing**:\n",
        "   - StyleGAN employs a progressively growing training regime, where the network starts with low-resolution images and gradually increases the resolution during training. This helps in stabilizing the training process and improving the quality of generated images.\n",
        "\n",
        "4. **Improved Image Quality**:\n",
        "   - StyleGAN produces high-quality images with fewer artifacts compared to traditional GANs. The use of adaptive instance normalization (AdaIN) from style transfer literature contributes to this improvement.\n",
        "\n",
        "5. **Disentangled Latent Representations**:\n",
        "   - The generator architecture allows for disentangled latent representations, meaning different dimensions of the latent space control different aspects of the generated images. This makes it easier to manipulate specific features without affecting others.\n",
        "\n",
        "### **Versions and Enhancements**:\n",
        "- **StyleGAN2**: Released in February 2020, it removes some characteristic artifacts and further improves image quality.\n",
        "- **StyleGAN3**: Released in June 2021, described as \"alias-free,\" it improves consistency between fine and coarse details in the generator.\n",
        "\n",
        "### **Applications**:\n",
        "StyleGANs have been used to create highly realistic images, including portraits of non-existent people, which have been showcased on websites like \"This Person Does Not Exist\". They are also used in various creative fields, such as art, fashion, and design, where precise control over visual attributes is essential.\n",
        "\n",
        "### **Unique Aspects**:\n",
        "- **Control Over Attributes**: The ability to control specific attributes of generated images makes StyleGANs highly versatile and powerful for various applications.\n",
        "- **High-Quality Outputs**: The architectural improvements lead to the generation of high-quality, realistic images with fewer artifacts.\n",
        "\n",
        "StyleGANs represent a significant advancement in the field of generative models, offering unprecedented control and quality in image generation. They continue to push the boundaries of what's possible with AI-driven creativity.\n",
        "\n",
        "Does this help clarify what makes StyleGANs unique?\n"
      ],
      "metadata": {
        "id": "R0-rTttIFLbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the role of noise in a GAN?\n"
      ],
      "metadata": {
        "id": "qDLtuxEHFSVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a Generative Adversarial Network (GAN), **noise** plays a crucial role as the starting point for generating new data. Here’s how it functions and its significance:\n",
        "\n",
        "### **Role of Noise in GANs**:\n",
        "\n",
        "1. **Input to the Generator**:\n",
        "   - The generator in a GAN takes a random noise vector (often sampled from a Gaussian or uniform distribution) as input. This noise vector, also called latent code, is the seed for generating new data instances, like images, text, or audio.\n",
        "\n",
        "2. **Diversity of Outputs**:\n",
        "   - Noise introduces variability in the generated data. By using different noise vectors, the generator can produce a wide range of diverse outputs. This ensures that the generator does not produce the same data repeatedly, fostering creativity and diversity in the outputs.\n",
        "\n",
        "3. **Latent Space Exploration**:\n",
        "   - The noise vector represents a point in the latent space. By sampling different points in this space, the generator can explore various features and patterns in the data distribution. This latent space can be mapped to specific attributes in the generated data, allowing for control over these features.\n",
        "\n",
        "4. **Random Initialization**:\n",
        "   - Noise serves as a random initial condition that the generator transforms into meaningful data. The random nature of the noise is essential for the generator to learn the complex mapping from noise to realistic data through training.\n",
        "\n",
        "### **Significance of Noise**:\n",
        "\n",
        "- **Enabling Variety**: Noise ensures that the generator can produce a wide variety of outputs, crucial for applications where diversity is important, such as image generation or creative content creation.\n",
        "- **Control and Manipulation**: By understanding how the latent space (derived from noise) maps to different features, users can manipulate the noise vector to control specific attributes of the generated data.\n",
        "- **Fundamental to Training**: Without noise, the generator would lack the initial randomness needed to explore and learn the data distribution. Noise is fundamental in driving the learning process in GANs.\n",
        "\n",
        "### **Example**:\n",
        "\n",
        "Imagine generating faces using a GAN. Each unique noise vector might produce different faces with varying features like hair color, eye shape, or facial expression. By adjusting the noise vector, one can control these features, demonstrating the crucial role of noise in generating diverse and realistic data.\n",
        "\n",
        "Noise is the spark that ignites the generative process in GANs, enabling them to produce a rich tapestry of synthetic data that can mimic the diversity found in real-world data."
      ],
      "metadata": {
        "id": "OErUxUDgFUbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How does the loss function in a WGAN improve training stability?\n"
      ],
      "metadata": {
        "id": "a5lpmMzsFY4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function in a Wasserstein Generative Adversarial Network (WGAN) improves training stability through several key mechanisms, primarily by addressing the limitations of traditional GANs. Here’s how it works:\n",
        "\n",
        "### **1. Wasserstein Distance (Earth Mover's Distance)**:\n",
        "- **Core Idea**: WGAN uses the Wasserstein distance to measure the difference between the real data distribution and the generated data distribution. This distance provides a smoother and more meaningful gradient for the generator.\n",
        "- **Mathematical Representation**:\n",
        "  \\[\n",
        "  W(P_r, P_g) = \\inf_{\\gamma \\in \\Pi(P_r, P_g)} \\mathbb{E}_{(x,y) \\sim \\gamma} [\\| x - y \\|]\n",
        "  \\]\n",
        "  Where \\( \\Pi(P_r, P_g) \\) represents the set of all joint distributions whose marginals are \\( P_r \\) and \\( P_g \\).\n",
        "\n",
        "### **2. Improved Gradient Flow**:\n",
        "- **Traditional GAN Loss**: Uses binary cross-entropy which can lead to vanishing or exploding gradients, making it hard for the generator to learn effectively.\n",
        "- **WGAN Loss**: Provides continuous and smoother gradients even when the discriminator is much better than the generator, allowing for more stable and reliable updates.\n",
        "\n",
        "### **3. Weight Clipping**:\n",
        "- **Purpose**: Enforces a Lipschitz constraint on the critic (discriminator), which is necessary for the Wasserstein distance calculation.\n",
        "- **Implementation**: Weights of the critic are clipped to a fixed range after each gradient update, ensuring that the gradients remain controlled and do not lead to instability.\n",
        "\n",
        "### **4. Critic Instead of Discriminator**:\n",
        "- **Role**: In WGANs, the discriminator is replaced by a critic, which does not classify data as real or fake but instead scores the realness of the data.\n",
        "- **Impact**: This scoring mechanism leads to better learning dynamics and helps avoid the collapse seen in traditional GANs where the discriminator might become too confident and provide little learning signal to the generator.\n",
        "\n",
        "### **5. Better Objective Function**:\n",
        "- **Traditional GAN**: Minimizes the JS divergence between real and generated data distributions, which can be problematic if the distributions do not overlap initially.\n",
        "- **WGAN**: Minimizes the Wasserstein distance, which provides meaningful gradients even when the real and generated data distributions have little overlap, leading to more stable training.\n",
        "\n",
        "### **Illustration of WGAN Loss Functions**:\n",
        "- **Critic Loss**:\n",
        "  \\[\n",
        "  \\text{Loss}_D = \\mathbb{E}_{\\mathbf{x} \\sim P_r} [D(\\mathbf{x})] - \\mathbb{E}_{\\mathbf{z} \\sim P_z} [D(G(\\mathbf{z}))]\n",
        "  \\]\n",
        "- **Generator Loss**:\n",
        "  \\[\n",
        "  \\text{Loss}_G = -\\mathbb{E}_{\\mathbf{z} \\sim P_z} [D(G(\\mathbf{z}))]\n",
        "  \\]\n",
        "\n",
        "By addressing the shortcomings of traditional GAN loss functions, WGANs significantly improve the stability and quality of the training process, enabling the generator to produce more realistic and diverse data."
      ],
      "metadata": {
        "id": "H4lz_a1CFf5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Describe the architecture of a typical GAN.\n"
      ],
      "metadata": {
        "id": "H7PjMsm0Fh78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture of a typical Generative Adversarial Network (GAN) consists of two main neural networks: the generator and the discriminator. Here’s a detailed look at their components and functions:\n",
        "\n",
        "### **1. Generator**:\n",
        "The generator is responsible for creating fake data that resembles the real data. It starts with a random noise vector and transforms it into a realistic data sample.\n",
        "\n",
        "- **Input Layer**: Accepts a random noise vector (usually sampled from a Gaussian distribution).\n",
        "- **Hidden Layers**: Consist of multiple fully connected or convolutional layers. These layers progressively refine the noise vector into a structured and detailed data sample.\n",
        "  - **Fully Connected Layers**: Often used in simple GANs, where each neuron in one layer is connected to every neuron in the next layer.\n",
        "  - **Convolutional Layers**: Used in more advanced GANs like DCGANs for better handling of spatial data (e.g., images). Transpose convolutional layers (or deconvolutional layers) are often used to upscale the data.\n",
        "- **Activation Functions**: Typically ReLU activations for hidden layers and Tanh activation for the output layer to ensure the generated data falls within the desired range.\n",
        "\n",
        "### **2. Discriminator**:\n",
        "The discriminator acts as a binary classifier that distinguishes between real data and fake data generated by the generator.\n",
        "\n",
        "- **Input Layer**: Accepts a data sample (either real or fake).\n",
        "- **Hidden Layers**: Similar to the generator, the discriminator has multiple fully connected or convolutional layers that analyze the input data.\n",
        "  - **Convolutional Layers**: For image data, convolutional layers are commonly used to extract features from the input.\n",
        "- **Activation Functions**: Typically Leaky ReLU activations in hidden layers to allow some gradient to pass through even when the neuron is not active.\n",
        "- **Output Layer**: Uses a sigmoid activation function to produce an output between 0 and 1, representing the probability that the input data is real.\n",
        "\n",
        "### **Training Process**:\n",
        "1. **Generator Training**: The generator produces fake data samples that are fed into the discriminator along with real data.\n",
        "2. **Discriminator Training**: The discriminator is trained to correctly classify real and fake data. It updates its weights to improve classification accuracy.\n",
        "3. **Adversarial Loop**: The generator updates its weights to create more realistic data that can fool the discriminator. This adversarial process continues iteratively.\n",
        "\n",
        "### **Simplified Diagram**:\n",
        "\n",
        "```\n",
        "Generator:\n",
        "Random Noise -> Dense/Conv Layers -> Fake Data\n",
        "\n",
        "Discriminator:\n",
        "Data Sample -> Dense/Conv Layers -> Classification (Real/Fake)\n",
        "```\n",
        "\n",
        "By training these two networks in tandem, the generator becomes better at producing realistic data, and the discriminator becomes better at identifying fake data. This adversarial relationship drives both networks to improve continuously, leading to the generation of highly realistic synthetic data. The magic of GANs lies in this dynamic, competitive interplay!"
      ],
      "metadata": {
        "id": "8iu0-3fBFjx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What challenges do GANS face during training, and how can they be addressed?\n"
      ],
      "metadata": {
        "id": "AwlJq1CIFsMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Generative Adversarial Networks (GANs) comes with several challenges. Here are some common issues and ways to address them:\n",
        "\n",
        "### **1. Training Instability**\n",
        "**Challenge**: GANs are prone to instability during training due to the adversarial nature of the two networks.\n",
        "**Solution**:\n",
        "- **Learning Rate Tuning**: Carefully adjust learning rates for both the generator and the discriminator.\n",
        "- **Batch Normalization**: Use batch normalization to stabilize training by normalizing inputs to each layer.\n",
        "- **Gradual Progression**: Use progressive growing of GANs (starting with low-resolution data and gradually increasing resolution).\n",
        "\n",
        "### **2. Mode Collapse**\n",
        "**Challenge**: The generator may produce a limited variety of outputs, ignoring some parts of the data distribution.\n",
        "**Solution**:\n",
        "- **Minibatch Discrimination**: Use minibatch discrimination to ensure the generator produces diverse outputs.\n",
        "- **Latent Space Regularization**: Regularize the latent space to encourage diversity in the generated samples.\n",
        "\n",
        "### **3. Vanishing or Exploding Gradients**\n",
        "**Challenge**: Gradients may vanish or explode, making it difficult for the generator to learn.\n",
        "**Solution**:\n",
        "- **Wasserstein GAN (WGAN)**: Use the WGAN framework, which provides smoother gradients and improves training stability.\n",
        "- **Spectral Normalization**: Apply spectral normalization to the weights of the discriminator to stabilize gradients.\n",
        "\n",
        "### **4. Evaluation Metrics**\n",
        "**Challenge**: Evaluating the quality of generated data is subjective and lacks universally accepted metrics.\n",
        "**Solution**:\n",
        "- **Inception Score (IS)** and **Frechet Inception Distance (FID)**: Use these metrics to evaluate the quality and diversity of generated images.\n",
        "- **Human Evaluation**: Complement quantitative metrics with human evaluation to assess the realism and relevance of generated data.\n",
        "\n",
        "### **5. Computational Resources**\n",
        "**Challenge**: GANs require significant computational resources for training.\n",
        "**Solution**:\n",
        "- **Efficient Architectures**: Use more efficient network architectures to reduce computational load.\n",
        "- **Cloud Services**: Leverage cloud-based computing resources for training.\n",
        "\n",
        "### **6. Sensitivity to Hyperparameters**\n",
        "**Challenge**: GAN performance is highly sensitive to hyperparameters.\n",
        "**Solution**:\n",
        "- **Hyperparameter Search**: Use systematic hyperparameter search techniques like grid search or random search.\n",
        "- **Automated Tuning**: Employ automated hyperparameter tuning tools to optimize the settings.\n",
        "\n",
        "### **7. Ethical and Bias Issues**\n",
        "**Challenge**: GANs can learn and propagate biases present in the training data.\n",
        "**Solution**:\n",
        "- **Diverse Datasets**: Use diverse and balanced datasets to train GANs.\n",
        "- **Bias Detection and Mitigation**: Implement techniques for detecting and mitigating biases in generated outputs.\n",
        "\n",
        "### **Addressing Challenges Summary**\n",
        "\n",
        "| **Challenge**              | **Solution**                                             |\n",
        "|----------------------------|----------------------------------------------------------|\n",
        "| Training Instability       | Learning rate tuning, batch normalization, gradual progression |\n",
        "| Mode Collapse              | Minibatch discrimination, latent space regularization    |\n",
        "| Vanishing/Exploding Gradients | WGAN, spectral normalization                          |\n",
        "| Evaluation Metrics         | Inception Score (IS), Frechet Inception Distance (FID), human evaluation |\n",
        "| Computational Resources    | Efficient architectures, cloud services                  |\n",
        "| Sensitivity to Hyperparameters | Hyperparameter search, automated tuning            |\n",
        "| Ethical and Bias Issues    | Diverse datasets, bias detection and mitigation          |\n",
        "\n",
        "By addressing these challenges, researchers and practitioners can improve the stability and effectiveness of GANs, leading to more reliable and realistic data generation. Each solution contributes to making GANs more robust and versatile in a variety of applications."
      ],
      "metadata": {
        "id": "tCKM9FM6Fv3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does DCGAN help improve image generation in GANS?\n"
      ],
      "metadata": {
        "id": "jsnyRkzcFxnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Convolutional Generative Adversarial Networks (DCGANs) are a significant improvement over traditional GANs, particularly for image generation. Here’s how DCGANs enhance the process:\n",
        "\n",
        "### **1. Convolutional Layers**:\n",
        "- **Traditional GANs**: Often use fully connected layers, which are not optimal for capturing spatial hierarchies in images.\n",
        "- **DCGANs**: Utilize convolutional and transposed convolutional layers (also known as deconvolutional layers), which are better suited for handling image data by preserving spatial relationships.\n",
        "\n",
        "### **2. Strided Convolutions**:\n",
        "- **Pooling Layers**: Traditional methods often use pooling layers for downsampling, which can lose spatial information.\n",
        "- **Strided Convolutions**: DCGANs replace pooling layers with strided convolutions for downsampling and upsampling, maintaining more of the image’s spatial structure and details.\n",
        "\n",
        "### **3. Batch Normalization**:\n",
        "- **Normalization**: DCGANs implement batch normalization in both the generator and discriminator networks. This technique normalizes the inputs to each layer, which stabilizes the learning process, accelerates training, and helps in avoiding issues like mode collapse.\n",
        "\n",
        "### **4. Activation Functions**:\n",
        "- **ReLU and Tanh**: DCGANs use ReLU activation functions in the hidden layers of the generator and Leaky ReLU in the discriminator to allow gradients to pass through, preventing the vanishing gradient problem. The output layer of the generator typically uses a Tanh activation function, which helps in producing images with values in the range of [-1, 1].\n",
        "\n",
        "### **5. Architectural Simplicity and Efficiency**:\n",
        "- **Reduced Complexity**: By using a simpler and more structured architecture with convolutional layers, DCGANs reduce the complexity and number of parameters, leading to more efficient training and better generalization.\n",
        "\n",
        "### **Illustration**:\n",
        "\n",
        "- **Generator**:\n",
        "  - Input: Random noise vector\n",
        "  - Hidden Layers: Transposed convolutional layers, batch normalization, ReLU activations\n",
        "  - Output Layer: Convolutional layer with Tanh activation\n",
        "\n",
        "- **Discriminator**:\n",
        "  - Input: Image (real or fake)\n",
        "  - Hidden Layers: Convolutional layers, batch normalization, Leaky ReLU activations\n",
        "  - Output Layer: Fully connected layer with sigmoid activation\n",
        "\n",
        "### **Impact of DCGANs**:\n",
        "DCGANs’ architectural enhancements significantly improve the quality of generated images. They capture finer details, maintain spatial coherence, and produce more realistic and higher-resolution images compared to traditional GANs. This makes DCGANs particularly effective for applications in computer vision, art, and media generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "bmxZxHTeFzg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the key differences between a traditional GAN and a StyleGAN?\n"
      ],
      "metadata": {
        "id": "O5Na-OMfF42N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The differences between a traditional Generative Adversarial Network (GAN) and a StyleGAN (Style Generative Adversarial Network) are significant and highlight the advancements in AI-generated image synthesis. Here are the key distinctions:\n",
        "\n",
        "### **1. Architecture**:\n",
        "- **Traditional GAN**:\n",
        "  - Uses simple fully connected layers or convolutional layers for the generator and discriminator.\n",
        "  - The generator directly maps noise vectors to the output image space without an intermediate step.\n",
        "- **StyleGAN**:\n",
        "  - Incorporates a more complex architecture with style-based layers, allowing for better control over different features of the generated image.\n",
        "  - Introduces an intermediate latent space (W space) between the input noise vector (Z space) and the generator, enabling more intuitive manipulation of image attributes.\n",
        "\n",
        "### **2. Latent Space Representation**:\n",
        "- **Traditional GAN**:\n",
        "  - Directly maps the noise vector (Z space) to the output image, limiting the control over specific features.\n",
        "- **StyleGAN**:\n",
        "  - Utilizes an intermediate latent space (W space) that decouples different features, allowing for fine-grained control over various aspects of the image, such as style and structure.\n",
        "\n",
        "### **3. Progressive Growing**:\n",
        "- **Traditional GAN**:\n",
        "  - Typically does not employ progressive growing, which can lead to training instability and lower quality images.\n",
        "- **StyleGAN**:\n",
        "  - Uses progressive growing, starting with low-resolution images and gradually increasing the resolution during training. This helps in stabilizing training and improving image quality.\n",
        "\n",
        "### **4. Style Mixing and Control**:\n",
        "- **Traditional GAN**:\n",
        "  - Lacks mechanisms for style mixing and detailed control over generated images.\n",
        "- **StyleGAN**:\n",
        "  - Introduces style mixing, where styles from different layers can be mixed to create images with varied features. This allows for precise control over individual aspects of the image, such as hair color, facial features, and background details.\n",
        "\n",
        "### **5. Training and Output Quality**:\n",
        "- **Traditional GAN**:\n",
        "  - Often faces issues like mode collapse, training instability, and limited image quality.\n",
        "- **StyleGAN**:\n",
        "  - Enhances training stability and generates higher-quality images with fewer artifacts. The architectural improvements lead to more realistic and detailed outputs.\n",
        "\n",
        "### **Illustration**:\n",
        "\n",
        "| Aspect                    | Traditional GAN                                       | StyleGAN                                                |\n",
        "|---------------------------|-------------------------------------------------------|---------------------------------------------------------|\n",
        "| **Architecture**          | Simple fully connected/convolutional layers          | Style-based layers with intermediate latent space (W space)|\n",
        "| **Latent Space**          | Direct mapping from Z to image                        | Intermediate W space for better attribute control        |\n",
        "| **Progressive Growing**   | Not typically used                                   | Uses progressive growing for stability and quality       |\n",
        "| **Style Mixing**          | Not available                                        | Enables style mixing for fine-grained control            |\n",
        "| **Training Stability**    | Can be unstable                                       | More stable training and higher-quality images          |\n",
        "\n",
        "### **Impact**:\n",
        "StyleGAN’s architectural enhancements and innovative features significantly improve the quality and control over generated images. This makes StyleGAN a preferred choice for applications requiring high-quality image synthesis, such as digital art, virtual reality, and synthetic data generation.\n",
        "\n",
        "These advancements in StyleGAN highlight the continuous evolution of GAN architectures, pushing the boundaries of what’s possible with AI-driven image generation."
      ],
      "metadata": {
        "id": "H71HmsJKF_q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How does the discriminator decide whether an image is real or fake in a GAN?\n"
      ],
      "metadata": {
        "id": "vcyZiNZnGJKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The discriminator in a Generative Adversarial Network (GAN) functions as a binary classifier that decides whether an input image is real or fake based on learned features. Here’s a detailed look at how the discriminator makes this decision:\n",
        "\n",
        "### **1. Input Processing**:\n",
        "- The discriminator receives an image as input, which can either be a real image from the training dataset or a fake image generated by the generator.\n",
        "\n",
        "### **2. Feature Extraction**:\n",
        "- **Convolutional Layers**: The discriminator uses convolutional layers to extract features from the input image. These layers capture various levels of detail, such as edges, textures, and more complex patterns.\n",
        "- **Activation Functions**: Often, Leaky ReLU activation functions are used in the hidden layers to ensure that gradients can flow even if the neurons are not fully activated.\n",
        "\n",
        "### **3. Downsampling**:\n",
        "- **Pooling/Strided Convolutions**: The discriminator typically uses pooling layers or strided convolutions to downsample the image, reducing its dimensionality while retaining important features.\n",
        "\n",
        "### **4. Fully Connected Layers**:\n",
        "- After convolutional and downsampling layers, the extracted features are fed into fully connected layers, which further process the features and combine them to form a final representation.\n",
        "\n",
        "### **5. Output Layer**:\n",
        "- The final layer of the discriminator is usually a fully connected layer with a sigmoid activation function, which outputs a probability between 0 and 1. This probability indicates how likely the discriminator thinks the image is real (close to 1) or fake (close to 0).\n",
        "\n",
        "### **6. Decision Making**:\n",
        "- Based on the output probability, the discriminator decides whether the input image is real or fake. If the output is closer to 1, the image is classified as real. If it is closer to 0, the image is classified as fake.\n",
        "\n",
        "### **Training Process**:\n",
        "- During training, the discriminator is updated based on its ability to correctly classify real and fake images. The loss function (such as binary cross-entropy) measures the accuracy of its predictions.\n",
        "- The generator is trained simultaneously to produce images that can fool the discriminator, aiming to maximize the discriminator’s output for fake images.\n",
        "\n",
        "### **Illustration of the Process**:\n",
        "\n",
        "1. **Input Image** -> Convolutional Layers -> Feature Maps\n",
        "2. Feature Maps -> Downsampling (Pooling/Strided Convolutions) -> Reduced Dimension Feature Maps\n",
        "3. Reduced Dimension Feature Maps -> Fully Connected Layers -> Final Representation\n",
        "4. Final Representation -> Sigmoid Activation -> Probability (Real/Fake)\n",
        "\n",
        "By continually refining its ability to extract and analyze features from images, the discriminator becomes better at distinguishing real data from the synthetic data generated by the generator. This adversarial training drives both networks to improve, resulting in highly realistic generated images."
      ],
      "metadata": {
        "id": "EYM8tjUSGKdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is the main advantage of using GANS in image generation?\n"
      ],
      "metadata": {
        "id": "3vRDSvfvGQYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main advantage of using Generative Adversarial Networks (GANs) in image generation is their remarkable ability to produce highly realistic and high-quality images. This is achieved through the adversarial training process, where the generator and discriminator continuously improve by challenging each other. Here are the key benefits:\n",
        "\n",
        "### **1. Realism and Quality**\n",
        "- **High-Fidelity Images**: GANs are capable of generating images that are often indistinguishable from real images to the human eye, thanks to the detailed and high-resolution features they learn to produce.\n",
        "\n",
        "### **2. Diversity of Outputs**\n",
        "- **Variety**: GANs can produce a wide range of diverse images from the same training dataset. By sampling different noise vectors, GANs generate varied outputs, ensuring a rich diversity in the generated images.\n",
        "\n",
        "### **3. Creativity and Flexibility**\n",
        "- **Creative Applications**: GANs enable creative processes, such as generating new art, fashion designs, and other visual content. Their flexibility allows for customization and controlled image synthesis, facilitating innovation in multiple fields.\n",
        "\n",
        "### **4. Data Augmentation**\n",
        "- **Enhanced Training Data**: GANs can be used to augment datasets by generating additional images, which is particularly useful in machine learning tasks where labeled data is scarce. This helps improve the performance of models trained on such datasets.\n",
        "\n",
        "### **5. Advancement in AI Research**\n",
        "- **Pushes Boundaries**: The continuous improvements and research in GAN technology push the boundaries of what is possible with AI, driving advancements in not only image generation but also in other areas such as video synthesis, 3D model generation, and more.\n",
        "\n",
        "### **Illustrative Example**:\n",
        "\n",
        "| **Advantage**       | **Description**                                               |\n",
        "|---------------------|---------------------------------------------------------------|\n",
        "| **Realism**         | Produces highly realistic and high-resolution images.        |\n",
        "| **Diversity**       | Generates a wide variety of images from the same dataset.    |\n",
        "| **Creativity**      | Enables innovative applications in art and design.            |\n",
        "| **Data Augmentation**| Enhances datasets by generating additional training images.   |\n",
        "| **Research**        | Advances AI capabilities and opens new possibilities.        |\n",
        "\n",
        "In essence, GANs revolutionize image generation by combining realism, diversity, and creative control, making them a powerful tool for both practical and innovative applications. Their influence extends beyond image synthesis, marking a significant leap in AI research and development."
      ],
      "metadata": {
        "id": "OCUe7VpRGScl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. How can GANs be used in real-world applications?\n"
      ],
      "metadata": {
        "id": "d5rTonuyGZMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative Adversarial Networks (GANs) have a multitude of fascinating and practical real-world applications across various domains. Here are some prominent examples:\n",
        "\n",
        "### **1. Image Synthesis and Enhancement**\n",
        "- **Art and Design**: GANs can generate high-quality, artistic images, providing new tools for artists and designers to create unique pieces.\n",
        "- **Photo Enhancement**: They can improve the resolution of images (super-resolution), deblur photos, and even colorize black-and-white images.\n",
        "\n",
        "### **2. Healthcare**\n",
        "- **Medical Imaging**: GANs can enhance medical images, such as MRI or CT scans, by improving their clarity and helping in the early detection of diseases.\n",
        "- **Drug Discovery**: They are used to generate molecular structures and simulate chemical reactions, aiding in the discovery of new drugs and treatments.\n",
        "\n",
        "### **3. Data Augmentation**\n",
        "- **Machine Learning**: GANs generate synthetic data to augment training datasets, which is especially useful when real data is scarce or expensive to collect. This improves the performance of machine learning models.\n",
        "\n",
        "### **4. Video and Animation**\n",
        "- **Deepfakes**: GANs can create realistic videos of individuals, which has both positive applications (e.g., film and entertainment) and ethical concerns.\n",
        "- **Animation**: They can generate lifelike animations and special effects, enhancing the realism in movies and games.\n",
        "\n",
        "### **5. Style Transfer**\n",
        "- **Fashion and Design**: GANs can transfer styles between images, such as applying the texture of one clothing item to another, aiding fashion designers in creating new designs.\n",
        "- **Art**: They enable the creation of artwork in the style of famous artists, opening new creative possibilities.\n",
        "\n",
        "### **6. Text-to-Image Synthesis**\n",
        "- **Advertising and Media**: GANs can generate images from textual descriptions, useful in creating marketing materials, visualizing concepts, and more.\n",
        "\n",
        "### **7. Security and Surveillance**\n",
        "- **Anomaly Detection**: GANs can identify unusual patterns in data, which is helpful in security applications, such as detecting fraudulent activities or intrusions.\n",
        "\n",
        "### **8. Personalized Content Creation**\n",
        "- **Gaming**: They can generate personalized avatars, game levels, and more, enhancing user experience in video games.\n",
        "- **Virtual Reality (VR)**: GANs can create realistic and immersive environments in virtual reality applications.\n",
        "\n",
        "### **Summary Table**:\n",
        "\n",
        "| **Application**              | **Description**                                               |\n",
        "|------------------------------|---------------------------------------------------------------|\n",
        "| **Image Synthesis**          | Artistic image creation, photo enhancement                    |\n",
        "| **Healthcare**               | Medical image enhancement, drug discovery                     |\n",
        "| **Data Augmentation**        | Generating synthetic data for training                        |\n",
        "| **Video and Animation**      | Creating deepfakes, lifelike animations                       |\n",
        "| **Style Transfer**           | Transferring styles in fashion, art                           |\n",
        "| **Text-to-Image Synthesis**  | Generating images from textual descriptions                   |\n",
        "| **Security and Surveillance**| Anomaly detection                                             |\n",
        "| **Personalized Content**     | Personalized gaming avatars, VR environments                  |\n",
        "\n",
        "These applications demonstrate the versatility and potential of GANs in transforming various industries by providing innovative solutions and enhancing existing processes. The creative and practical uses of GANs continue to expand, pushing the boundaries of what’s possible with AI technology."
      ],
      "metadata": {
        "id": "UKcuxbCLGaon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. What is Mode Collapse in GANS, and how can it be prevented?\n"
      ],
      "metadata": {
        "id": "z8ZmostmGjAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mode collapse** is a common problem in Generative Adversarial Networks (GANs) where the generator produces a limited variety of outputs, focusing on a few modes of the data distribution and ignoring others. This leads to a lack of diversity in the generated data, with the generator often repeating similar outputs. Here’s a more detailed look at mode collapse and strategies to prevent it:\n",
        "\n",
        "### **Mode Collapse Explained**:\n",
        "\n",
        "1. **What Happens**:\n",
        "   - The generator finds a way to produce outputs that can consistently fool the discriminator, but these outputs are not diverse and represent only a small subset of the real data distribution.\n",
        "\n",
        "2. **Symptoms**:\n",
        "   - Lack of variation in the generated images.\n",
        "   - Repetition of the same or very similar images across different runs.\n",
        "\n",
        "### **Strategies to Prevent Mode Collapse**:\n",
        "\n",
        "1. **Minibatch Discrimination**:\n",
        "   - **Concept**: This technique introduces minibatch features that depend on the entire batch of generated data, allowing the discriminator to see multiple samples at once and penalize the generator for lack of diversity.\n",
        "   - **Implementation**: Minibatch discrimination layers are added to the discriminator to evaluate the distance between samples in the same batch, promoting diversity.\n",
        "\n",
        "2. **Unrolled GANs**:\n",
        "   - **Concept**: Unrolling the optimization of the discriminator over several steps can help the generator anticipate future gradients, making it harder for mode collapse to occur.\n",
        "   - **Implementation**: During training, the generator takes into account the updates that the discriminator will make in future iterations, effectively “unrolling” the optimization process.\n",
        "\n",
        "3. **Feature Matching**:\n",
        "   - **Concept**: Instead of only trying to fool the discriminator, the generator also tries to match the statistics of the features of the real data.\n",
        "   - **Implementation**: The generator's loss includes a term that measures the difference between the features of the real and generated data, ensuring that the generator captures a wider variety of features.\n",
        "\n",
        "4. **Adding Noise**:\n",
        "   - **Concept**: Introducing noise to the discriminator's inputs can prevent it from becoming too good, encouraging the generator to explore a broader range of outputs.\n",
        "   - **Implementation**: Noise is added to the inputs of the discriminator during training, which can help in maintaining a dynamic learning process.\n",
        "\n",
        "5. **Auxiliary Classifier GAN (AC-GAN)**:\n",
        "   - **Concept**: AC-GANs include an auxiliary classifier that predicts class labels for the generated images, promoting diversity and making mode collapse less likely.\n",
        "   - **Implementation**: The generator is conditioned on class labels, and the discriminator not only predicts whether an image is real or fake but also assigns a class label to it.\n",
        "\n",
        "### **Summary Table**:\n",
        "\n",
        "| **Technique**               | **Description**                                                           |\n",
        "|-----------------------------|---------------------------------------------------------------------------|\n",
        "| **Minibatch Discrimination**| Evaluates distances between samples in a batch to promote diversity.     |\n",
        "| **Unrolled GANs**           | Considers future updates to the discriminator to prevent mode collapse.   |\n",
        "| **Feature Matching**        | Matches feature statistics of real and generated data.                    |\n",
        "| **Adding Noise**            | Introduces noise to the discriminator's inputs to maintain learning dynamics. |\n",
        "| **Auxiliary Classifier GAN**| Includes a classifier to predict class labels, enhancing diversity.       |\n",
        "\n",
        "By applying these strategies, researchers and practitioners can mitigate mode collapse and ensure that GANs generate a diverse and comprehensive range of outputs, making the models more robust and effective for various applications."
      ],
      "metadata": {
        "id": "KCLIcSWlGkcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "ZuZDrPAcGtCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "3tV8nBJJGwSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "dBaTUTKsGwXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "kkRfP7pHGwez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Practical**"
      ],
      "metadata": {
        "id": "o_FGEP9vGx-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Implement a simple GAN architecture to generate random images (like noise or basic shapes) using TensorFlow/Keras.\n"
      ],
      "metadata": {
        "id": "MrawsB1YGzXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the Generator\n",
        "def build_generator():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_dim=100))  # Input noise (latent vector)\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(1024, activation='relu'))\n",
        "    model.add(layers.Dense(28 * 28 * 1, activation='sigmoid'))  # Output image size (28x28)\n",
        "    model.add(layers.Reshape((28, 28, 1)))  # Reshape into image\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator\n",
        "def build_discriminator():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=(28, 28, 1)))  # Flatten the image into a 1D vector\n",
        "    model.add(layers.Dense(1024, activation='relu'))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))  # Binary output: real or fake\n",
        "    return model\n",
        "\n",
        "# Define the GAN model that combines both the generator and discriminator\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False  # During GAN training, freeze the discriminator\n",
        "    model = models.Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n",
        "\n",
        "# Compile and train the GAN\n",
        "def compile_models(generator, discriminator, gan):\n",
        "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    gan.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training loop\n",
        "def train_gan(generator, discriminator, gan, epochs=10000, batch_size=64, save_interval=1000):\n",
        "    # Load MNIST dataset for real images (we use grayscale images of 28x28)\n",
        "    (X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "    X_train = X_train / 127.5 - 1.0  # Normalize to [-1, 1]\n",
        "    X_train = np.expand_dims(X_train, axis=3)  # Add channel dimension\n",
        "\n",
        "    half_batch = batch_size // 2\n",
        "    for epoch in range(epochs):\n",
        "        # Train discriminator\n",
        "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "        real_images = X_train[idx]\n",
        "\n",
        "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "        fake_images = generator.predict(noise)\n",
        "\n",
        "        real_labels = np.ones((half_batch, 1))  # Real labels = 1\n",
        "        fake_labels = np.zeros((half_batch, 1))  # Fake labels = 0\n",
        "\n",
        "        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        valid_labels = np.ones((batch_size, 1))  # Generator aims to fool the discriminator\n",
        "\n",
        "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "        # Print the progress\n",
        "        if epoch % save_interval == 0:\n",
        "            print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
        "            save_generated_images(epoch)\n",
        "\n",
        "# Save generated images\n",
        "def save_generated_images(epoch, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
        "    noise = np.random.normal(0, 1, (examples, 100))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1]\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(examples):\n",
        "        plt.subplot(dim[0], dim[1], i + 1)\n",
        "        plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"gan_generated_image_epoch_{epoch}.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Main function to run the GAN\n",
        "if __name__ == \"__main__\":\n",
        "    # Build the models\n",
        "    generator = build_generator()\n",
        "    discriminator = build_discriminator()\n",
        "    gan = build_gan(generator, discriminator)\n",
        "\n",
        "    # Compile models\n",
        "    compile_models(generator, discriminator, gan)\n",
        "\n",
        "    # Train the GAN\n",
        "    train_gan(generator, discriminator, gan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bF33hqgNHAoM",
        "outputId": "428f2fa3-02a5-4406-d5a1-5dfe8f7abcef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 757ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:75: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [D loss: 0.725780725479126 | D accuracy: 32.8125] [G loss: [array(0.75359505, dtype=float32), array(0.75359505, dtype=float32), array(1., dtype=float32), array(1., dtype=float32)]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7de7346324d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7de733d3d870> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-42166f783a27>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Train the GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-42166f783a27>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mvalid_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Generator aims to fool the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Print the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement the discriminator for a GAN with an image input of shape (28,28).\n"
      ],
      "metadata": {
        "id": "keMg_gRcG2Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_discriminator(input_shape=(28, 28, 1)):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # First convolutional layer\n",
        "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=input_shape))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))  # LeakyReLU activation\n",
        "\n",
        "    # Second convolutional layer\n",
        "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "\n",
        "    # Third convolutional layer\n",
        "    model.add(layers.Conv2D(256, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "\n",
        "    # Fourth convolutional layer\n",
        "    model.add(layers.Conv2D(512, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "\n",
        "    # Flatten the output to a 1D vector\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense layer for the final output\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))  # Output is a probability (real or fake)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "discriminator = build_discriminator()\n",
        "discriminator.summary()  # Display the model architecture\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "kAKnSEppHLpQ",
        "outputId": "be69bbb4-5d3a-43f6-89a1-ccfc3af8099d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m73,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m295,168\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m1,180,160\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │           \u001b[38;5;34m2,049\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,551,873\u001b[0m (5.92 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,551,873</span> (5.92 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,551,873\u001b[0m (5.92 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,551,873</span> (5.92 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Train the generator to produce simple digits (using noise as input) and plot the generated images.\n"
      ],
      "metadata": {
        "id": "Mjbx5RSfHN_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the Generator model\n",
        "def build_generator(latent_dim=100):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_dim=latent_dim))\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(1024, activation='relu'))\n",
        "    model.add(layers.Dense(28 * 28 * 1, activation='sigmoid'))  # Output a 28x28 image\n",
        "    model.add(layers.Reshape((28, 28, 1)))  # Reshape into image shape\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator model\n",
        "def build_discriminator(input_shape=(28, 28, 1)):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=input_shape))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Conv2D(256, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Conv2D(512, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))  # Output a probability (real or fake)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the GAN model that combines both the generator and discriminator\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False  # Freeze the discriminator during generator training\n",
        "    model = models.Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n",
        "\n",
        "# Compile and train the GAN\n",
        "def compile_models(generator, discriminator, gan):\n",
        "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    gan.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training loop\n",
        "def train_gan(generator, discriminator, gan, epochs=10000, batch_size=64, save_interval=1000):\n",
        "    # Load MNIST dataset for real images (28x28 grayscale)\n",
        "    (X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "    X_train = X_train / 127.5 - 1.0  # Normalize to [-1, 1]\n",
        "    X_train = np.expand_dims(X_train, axis=3)  # Add channel dimension for grayscale images\n",
        "\n",
        "    half_batch = batch_size // 2\n",
        "    for epoch in range(epochs):\n",
        "        # Train discriminator with real images\n",
        "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "        real_images = X_train[idx]\n",
        "\n",
        "        # Generate fake images\n",
        "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "        fake_images = generator.predict(noise)\n",
        "\n",
        "        # Labels for real and fake images\n",
        "        real_labels = np.ones((half_batch, 1))  # Real labels = 1\n",
        "        fake_labels = np.zeros((half_batch, 1))  # Fake labels = 0\n",
        "\n",
        "        # Train the discriminator (real and fake images)\n",
        "        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train the generator to fool the discriminator\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))  # Random noise\n",
        "        valid_labels = np.ones((batch_size, 1))  # Generator wants to fool the discriminator to output 1\n",
        "\n",
        "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "        # Print the progress and save generated images at intervals\n",
        "        if epoch % save_interval == 0:\n",
        "            print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
        "            save_generated_images(epoch)\n",
        "\n",
        "# Save generated images to plot\n",
        "def save_generated_images(epoch, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
        "    noise = np.random.normal(0, 1, (examples, 100))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1]\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(examples):\n",
        "        plt.subplot(dim[0], dim[1], i + 1)\n",
        "        plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"gan_generated_image_epoch_{epoch}.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Main function to run the GAN\n",
        "if __name__ == \"__main__\":\n",
        "    # Build the models\n",
        "    generator = build_generator()\n",
        "    discriminator = build_discriminator()\n",
        "    gan = build_gan(generator, discriminator)\n",
        "\n",
        "    # Compile the models\n",
        "    compile_models(generator, discriminator, gan)\n",
        "\n",
        "    # Train the GAN\n",
        "    train_gan(generator, discriminator, gan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "e_wj_TmhHWdP",
        "outputId": "a51e0fc5-2985-4e0f-b6f0-4becca9ebd40"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "0 [D loss: 0.7033840417861938 | D accuracy: 25.0] [G loss: [array(0.69761527, dtype=float32), array(0.69761527, dtype=float32), array(0., dtype=float32), array(0., dtype=float32)]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d6766eccc2e1>\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Train the GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-d6766eccc2e1>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, gan, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Train the discriminator (real and fake images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m   function = trace_function(\n\u001b[0m\u001b[1;32m    133\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       return api.converted_call(\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mone_step_on_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;34m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             outputs = self.distribute_strategy.run(\n\u001b[0m\u001b[1;32m    122\u001b[0m                 \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1671\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1672\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1673\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3261\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3263\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3265\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   4059\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4060\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4061\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4063\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1332\u001b[0m           {\"PartitionedCall\": self._get_gradient_function(),\n\u001b[1;32m   1333\u001b[0m            \"StatefulPartitionedCall\": self._get_gradient_function()}):\n\u001b[0;32m-> 1334\u001b[0;31m         \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_with_tangents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0mforward_backward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    255\u001b[0m             )\n\u001b[1;32m    256\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             outputs = make_call_op_in_graph(\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mmake_call_op_in_graph\u001b[0;34m(atomic, tensor_inputs, context_call_attrs)\u001b[0m\n\u001b[1;32m    430\u001b[0m   \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_function_recursive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matomic\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m   op = partitioned_call_op(  # pytype: disable=wrong-arg-types  # always-use-property-annotation\n\u001b[0m\u001b[1;32m    433\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matomic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mpartitioned_call_op\u001b[0;34m(name, args, is_stateful, tout, config, executor_type, xla_compile_attr)\u001b[0m\n\u001b[1;32m    380\u001b[0m   \u001b[0;31m# The generated binding returns an empty list for functions that don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m   \u001b[0;31m# return any Tensors, hence the need to use `create_op` directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m   tin_attr = attr_value_pb2.AttrValue(\n\u001b[1;32m    384\u001b[0m       list=attr_value_pb2.AttrValue.ListValue(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    380\u001b[0m   \u001b[0;31m# The generated binding returns an empty list for functions that don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m   \u001b[0;31m# return any Tensors, hence the need to use `create_op` directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m   tin_attr = attr_value_pb2.AttrValue(\n\u001b[1;32m    384\u001b[0m       list=attr_value_pb2.AttrValue.ListValue(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    714\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    207\u001b[0m   \u001b[0moverload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__tf_tensor__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moverload\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moverload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#  pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversion_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;34m\"building a function.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                 name=name))\n\u001b[0;32m--> 607\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tf_tensor__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mcapture\u001b[0;34m(self, tensor, name, shape)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_captures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_validate_in_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/capture/capture_container.py\u001b[0m in \u001b[0;36mcapture_by_value\u001b[0;34m(self, graph, tensor, name)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0;31m# Large EagerTensors and resources are captured with Placeholder ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_placeholder_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/capture/capture_container.py\u001b[0m in \u001b[0;36m_create_placeholder_helper\u001b[0;34m(self, graph, tensor, name)\u001b[0m\n\u001b[1;32m    283\u001b[0m           \u001b[0mcomposite_device_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomposite_device_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m       )\n\u001b[0;32m--> 285\u001b[0;31m       \u001b[0mplaceholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaceholder_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m       self.add_or_replace(\n\u001b[1;32m    287\u001b[0m           \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexternal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_by_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36mplaceholder_value\u001b[0;34m(self, placeholder_context)\u001b[0m\n\u001b[1;32m   1017\u001b[0m       \u001b[0;31m# capturing placeholders outside of any control flow context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mcontext_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m         \u001b[0mplaceholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m       \u001b[0mplaceholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m_graph_placeholder\u001b[0;34m(self, graph, name)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdtype_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m       op = graph._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   1060\u001b[0m           \u001b[0;34m\"Placeholder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m           attrs=attrs, name=name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m       \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m     return super()._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         compute_device)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   2680\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m       ret = Operation.from_node_def(\n\u001b[0m\u001b[1;32m   2683\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mfrom_node_def\u001b[0;34m(cls, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1066\u001b[0m   \"\"\"\n\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m   def from_node_def(\n\u001b[1;32m   1070\u001b[0m       \u001b[0mcls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOperationType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Implement WGAN by modifying the loss function in the GAN.\n"
      ],
      "metadata": {
        "id": "No97InvuHasj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the Generator model\n",
        "def build_generator(latent_dim=100):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_dim=latent_dim))\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(1024, activation='relu'))\n",
        "    model.add(layers.Dense(28 * 28 * 1, activation='tanh'))  # Output a 28x28 image\n",
        "    model.add(layers.Reshape((28, 28, 1)))  # Reshape into image shape\n",
        "    return model\n",
        "\n",
        "# Define the Critic model (Discriminator for WGAN)\n",
        "def build_critic(input_shape=(28, 28, 1)):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=input_shape))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Conv2D(256, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Conv2D(512, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))  # Output a scalar score\n",
        "    return model\n",
        "\n",
        "# Define the WGAN model that combines both the generator and critic\n",
        "def build_wgan(generator, critic):\n",
        "    critic.trainable = False  # Freeze the critic during generator training\n",
        "    model = models.Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(critic)\n",
        "    return model\n",
        "\n",
        "# Compile and train the WGAN\n",
        "def compile_models(generator, critic, wgan):\n",
        "    critic.compile(optimizer='adam', loss='mse')  # Mean Squared Error loss for critic\n",
        "    wgan.compile(optimizer='adam', loss='mse')  # Mean Squared Error loss for WGAN\n",
        "\n",
        "# Clip critic weights to enforce Lipschitz constraint\n",
        "def clip_critic_weights(critic, clip_value=0.01):\n",
        "    for layer in critic.layers:\n",
        "        if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Dense):\n",
        "            weights = layer.get_weights()\n",
        "            weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n",
        "            layer.set_weights(weights)\n",
        "\n",
        "# Training loop\n",
        "def train_wgan(generator, critic, wgan, epochs=10000, batch_size=64, save_interval=1000):\n",
        "    # Load MNIST dataset for real images (28x28 grayscale)\n",
        "    (X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "    X_train = X_train / 127.5 - 1.0  # Normalize to [-1, 1]\n",
        "    X_train = np.expand_dims(X_train, axis=3)  # Add channel dimension for grayscale images\n",
        "\n",
        "    half_batch = batch_size // 2\n",
        "    for epoch in range(epochs):\n",
        "        # Train the critic with real images\n",
        "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "        real_images = X_train[idx]\n",
        "\n",
        "        # Generate fake images\n",
        "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "        fake_images = generator.predict(noise)\n",
        "\n",
        "        # Train the critic (real and fake images)\n",
        "        critic_loss_real = critic.train_on_batch(real_images, np.ones((half_batch, 1)))  # Real images score = 1\n",
        "        critic_loss_fake = critic.train_on_batch(fake_images, -np.ones((half_batch, 1)))  # Fake images score = -1\n",
        "        critic_loss = 0.5 * np.add(critic_loss_real, critic_loss_fake)\n",
        "\n",
        "        # Clip critic weights to enforce Lipschitz constraint\n",
        "        clip_critic_weights(critic)\n",
        "\n",
        "        # Train the generator every few steps\n",
        "        if epoch % 5 == 0:\n",
        "            noise = np.random.normal(0, 1, (batch_size, 100))  # Random noise\n",
        "            valid_labels = np.ones((batch_size, 1))  # Generator wants to fool the critic to output 1\n",
        "            g_loss = wgan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "        # Print the progress and save generated images at intervals\n",
        "        if epoch % save_interval == 0:\n",
        "            print(f\"{epoch} [Critic loss: {critic_loss}] [Generator loss: {g_loss}]\")\n",
        "            save_generated_images(epoch)\n",
        "\n",
        "# Save generated images to plot\n",
        "def save_generated_images(epoch, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
        "    noise = np.random.normal(0, 1, (examples, 100))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1]\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(examples):\n",
        "        plt.subplot(dim[0], dim[1], i + 1)\n",
        "        plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"wgan_generated_image_epoch_{epoch}.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Main function to run the WGAN\n",
        "if __name__ == \"__main__\":\n",
        "    # Build the models\n",
        "    generator = build_generator()\n",
        "    critic = build_critic()\n",
        "    wgan = build_wgan(generator, critic)\n",
        "\n",
        "    # Compile the models\n",
        "    compile_models(generator, critic, wgan)\n",
        "\n",
        "    # Train the WGAN\n",
        "    train_wgan(generator, critic, wgan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "J6fMo19OHcg_",
        "outputId": "4211466e-40b6-4724-cdcd-c7ab88733ab6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n",
            "0 [Critic loss: 1.010894775390625] [Generator loss: [array(1.0071037, dtype=float32), array(1.0071037, dtype=float32)]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6668827e5599>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# Train the WGAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mtrain_wgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-6668827e5599>\u001b[0m in \u001b[0;36mtrain_wgan\u001b[0;34m(generator, critic, wgan, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Generate fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Train the critic (real and fake images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36menumerate_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 for step in range(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 709\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    746\u001b[0m             self._flat_output_types)\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3479\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3480\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Use a trained generator to generate a batch of fake images and display them.\n"
      ],
      "metadata": {
        "id": "hyHhZFBrHpBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to generate and display fake images using the trained generator\n",
        "def generate_and_display_fake_images(generator, batch_size=10, noise_dim=100, figsize=(10, 1)):\n",
        "    # Generate random noise\n",
        "    noise = np.random.normal(0, 1, (batch_size, noise_dim))  # noise_dim = 100 (as per previous code)\n",
        "\n",
        "    # Generate fake images using the trained generator\n",
        "    generated_images = generator.predict(noise)\n",
        "\n",
        "    # Rescale the images to the range [0, 1] for visualization\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Since we used tanh activation, rescale to [0, 1]\n",
        "\n",
        "    # Display the generated images\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(batch_size):\n",
        "        plt.subplot(1, batch_size, i + 1)\n",
        "        plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Assuming you have a trained 'generator' model, call the function to generate and display images:\n",
        "generate_and_display_fake_images(generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "CAnGvawYHsjg",
        "outputId": "a3fb64e1-9a5f-4b39-92b1-a6ac3ea710d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x100 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Create a StyleGAN-inspired architecture that outputs high-resolution images."
      ],
      "metadata": {
        "id": "eCrk8l6WIGMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PixelNormalization(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "class EqualizedConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
        "        self.scale = torch.sqrt(2 / (in_channels * kernel_size * kernel_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.conv2d(\n",
        "            x,\n",
        "            self.weight * self.scale,\n",
        "            self.bias,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "class StyleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, style_dim):\n",
        "        super().__init__()\n",
        "        self.style_mod = nn.Sequential(\n",
        "            nn.Linear(style_dim, in_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_channels, in_channels)\n",
        "        )\n",
        "\n",
        "        self.conv1 = EqualizedConv2d(in_channels, out_channels, kernel_size=3)\n",
        "        self.conv2 = EqualizedConv2d(out_channels, out_channels, kernel_size=3)\n",
        "\n",
        "        self.pixel_norm = PixelNormalization()\n",
        "        self.lrelu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x, style):\n",
        "        # Style modulation\n",
        "        style_mod = self.style_mod(style)\n",
        "        x = x * style_mod.view(x.size(0), x.size(1), 1, 1)\n",
        "\n",
        "        # Convolution and normalization\n",
        "        x = self.pixel_norm(self.lrelu(self.conv1(x)))\n",
        "        x = self.pixel_norm(self.lrelu(self.conv2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=512, n_styles=18, img_channels=3):\n",
        "        super().__init__()\n",
        "        self.n_styles = n_styles\n",
        "\n",
        "        # Initial constant input\n",
        "        self.initial_const = nn.Parameter(torch.randn(1, 512, 4, 4))\n",
        "\n",
        "        # Progressive growing blocks\n",
        "        self.style_blocks = nn.ModuleList([\n",
        "            StyleBlock(512, 512, latent_dim) for _ in range(n_styles)\n",
        "        ])\n",
        "\n",
        "        # To RGB conversion at each resolution\n",
        "        self.to_rgbs = nn.ModuleList([\n",
        "            EqualizedConv2d(512, img_channels, kernel_size=1)\n",
        "            for _ in range(n_styles)\n",
        "        ])\n",
        "\n",
        "        # Upsampling\n",
        "        self.upsampler = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "    def forward(self, style, alpha=1.0):\n",
        "        # Initial constant input\n",
        "        x = self.initial_const.repeat(style.size(0), 1, 1, 1)\n",
        "\n",
        "        # Progressive style application\n",
        "        rgb_outputs = []\n",
        "        for i, (style_block, to_rgb) in enumerate(zip(self.style_blocks, self.to_rgbs)):\n",
        "            # Apply style\n",
        "            x = style_block(x, style[:, i])\n",
        "\n",
        "            # Upsample except for first block\n",
        "            if i > 0:\n",
        "                x = self.upsampler(x)\n",
        "\n",
        "            # Convert to RGB\n",
        "            rgb = to_rgb(x)\n",
        "            rgb_outputs.append(rgb)\n",
        "\n",
        "        # Blend resolutions during training\n",
        "        if len(rgb_outputs) > 1 and alpha < 1.0:\n",
        "            return (1 - alpha) * rgb_outputs[-2] + alpha * rgb_outputs[-1]\n",
        "\n",
        "        return rgb_outputs[-1]\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Progressive growing discriminator blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                EqualizedConv2d(img_channels, 64, kernel_size=1),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                EqualizedConv2d(64, 64, kernel_size=3, padding=1),\n",
        "                nn.LeakyReLU(0.2)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                EqualizedConv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                EqualizedConv2d(128, 128, kernel_size=3, padding=1),\n",
        "                nn.LeakyReLU(0.2)\n",
        "            ),\n",
        "            # Additional blocks can be added for higher resolutions\n",
        "        ])\n",
        "\n",
        "        # Final classification layer\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4, 1),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, alpha=1.0):\n",
        "        # Progressive growing\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "\n",
        "            # Optional alpha blending between resolutions\n",
        "            if i > 0 and alpha < 1.0:\n",
        "                x_prev = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "                x = (1 - alpha) * x_prev + alpha * x\n",
        "\n",
        "        return self.final_layer(x)\n",
        "\n",
        "# Example usage\n",
        "def generate_high_res_images():\n",
        "    # Instantiate generator\n",
        "    generator = Generator()\n",
        "\n",
        "    # Generate random latent vectors\n",
        "    latent_vectors = torch.randn(4, 18, 512)  # 4 images, 18 style layers\n",
        "\n",
        "    # Generate images\n",
        "    generated_images = generator(latent_vectors)\n",
        "\n",
        "    return generated_images\n",
        "\n",
        "# Training configuration example\n",
        "class StyleGANTrainer:\n",
        "    def __init__(self, lr=0.001):\n",
        "        self.generator = Generator()\n",
        "        self.discriminator = Discriminator()\n",
        "\n",
        "        # Optimizers\n",
        "        self.g_optimizer = torch.optim.Adam(\n",
        "            self.generator.parameters(),\n",
        "            lr=lr,\n",
        "            betas=(0.0, 0.99)\n",
        "        )\n",
        "        self.d_optimizer = torch.optim.Adam(\n",
        "            self.discriminator.parameters(),\n",
        "            lr=lr,\n",
        "            betas=(0.0, 0.99)\n",
        "        )\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Generate fake images\n",
        "        z = torch.randn(real_images.size(0), 18, 512)\n",
        "        fake_images = self.generator(z)\n",
        "\n",
        "        # Discriminator forward pass\n",
        "        real_scores = self.discriminator(real_images)\n",
        "        fake_scores = self.discriminator(fake_images)\n",
        "\n",
        "        # Adversarial loss\n",
        "        d_loss = F.softplus(fake_scores) + F.softplus(-real_scores)\n",
        "        g_loss = F.softplus(-fake_scores)\n",
        "\n",
        "        # Backpropagation\n",
        "        self.d_optimizer.zero_grad()\n",
        "        d_loss.backward()\n",
        "        self.d_optimizer.step()\n",
        "\n",
        "        self.g_optimizer.zero_grad()\n",
        "        g_loss.backward()\n",
        "        self.g_optimizer.step()\n",
        "\n",
        "        return d_loss.item(), g_loss.item()\n",
        "\n",
        "# Advanced features demonstration\n",
        "def style_mixing_experiment():\n",
        "    generator = Generator()\n",
        "\n",
        "    # Generate two different latent vectors\n",
        "    z1 = torch.randn(1, 18, 512)\n",
        "    z2 = torch.randn(1, 18, 512)\n",
        "\n",
        "    # Style mixing: use first half of styles from z1, second from z2\n",
        "    mixed_z = torch.cat([z1[:, :9], z2[:, 9:]], dim=1)\n",
        "\n",
        "    # Generate image with mixed styles\n",
        "    mixed_image = generator(mixed_z)\n",
        "\n",
        "    return mixed_image\n",
        "\n",
        "# Advanced feature: noise injection\n",
        "class NoiseInjection(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
        "\n",
        "    def forward(self, x, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device)\n",
        "        return x + self.weight * noise"
      ],
      "metadata": {
        "id": "lqktTM40Icak"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Implement the Wasserstein loss function for GAN training.\n"
      ],
      "metadata": {
        "id": "3zE3BGxZI_VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Wasserstein Loss for the Discriminator\n",
        "def wasserstein_discriminator_loss(real_scores, fake_scores):\n",
        "    \"\"\"\n",
        "    Computes the Wasserstein loss for the discriminator.\n",
        "\n",
        "    Args:\n",
        "        real_scores (Tensor): Discriminator's output for real samples.\n",
        "        fake_scores (Tensor): Discriminator's output for fake samples.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Wasserstein discriminator loss.\n",
        "    \"\"\"\n",
        "    return -torch.mean(real_scores) + torch.mean(fake_scores)\n",
        "\n",
        "# Wasserstein Loss for the Generator\n",
        "def wasserstein_generator_loss(fake_scores):\n",
        "    \"\"\"\n",
        "    Computes the Wasserstein loss for the generator.\n",
        "\n",
        "    Args:\n",
        "        fake_scores (Tensor): Discriminator's output for fake samples.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Wasserstein generator loss.\n",
        "    \"\"\"\n",
        "    return -torch.mean(fake_scores)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Simulated discriminator outputs\n",
        "    real_scores = torch.tensor([1.2, 1.5, 1.3])\n",
        "    fake_scores = torch.tensor([-0.8, -1.0, -0.5])\n",
        "\n",
        "    # Compute losses\n",
        "    d_loss = wasserstein_discriminator_loss(real_scores, fake_scores)\n",
        "    g_loss = wasserstein_generator_loss(fake_scores)\n",
        "\n",
        "    print(f\"Discriminator Loss: {d_loss.item()}\")\n",
        "    print(f\"Generator Loss: {g_loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm-c3KRaJB4R",
        "outputId": "57d1e2a9-b00b-4369-d706-9806c60f1e2a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator Loss: -2.0999999046325684\n",
            "Generator Loss: 0.7666666507720947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a function to modify the discriminator to include a dropout layer with a rate of 0.4 and print the configurations.\n"
      ],
      "metadata": {
        "id": "td71TBZuJ2rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PixelNormalization(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "class EqualizedConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
        "        self.scale = torch.sqrt(2 / (in_channels * kernel_size * kernel_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.conv2d(\n",
        "            x,\n",
        "            self.weight * self.scale,\n",
        "            self.bias,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "class StyleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, style_dim):\n",
        "        super().__init__()\n",
        "        self.style_mod = nn.Sequential(\n",
        "            nn.Linear(style_dim, in_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_channels, in_channels)\n",
        "        )\n",
        "\n",
        "        self.conv1 = EqualizedConv2d(in_channels, out_channels, kernel_size=3)\n",
        "        self.conv2 = EqualizedConv2d(out_channels, out_channels, kernel_size=3)\n",
        "\n",
        "        self.pixel_norm = PixelNormalization()\n",
        "        self.lrelu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x, style):\n",
        "        # Style modulation\n",
        "        style_mod = self.style_mod(style)\n",
        "        x = x * style_mod.view(x.size(0), x.size(1), 1, 1)\n",
        "\n",
        "        # Convolution and normalization\n",
        "        x = self.pixel_norm(self.lrelu(self.conv1(x)))\n",
        "        x = self.pixel_norm(self.lrelu(self.conv2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=512, n_styles=18, img_channels=3):\n",
        "        super().__init__()\n",
        "        self.n_styles = n_styles\n",
        "\n",
        "        # Initial constant input\n",
        "        self.initial_const = nn.Parameter(torch.randn(1, 512, 4, 4))\n",
        "\n",
        "        # Progressive growing blocks\n",
        "        self.style_blocks = nn.ModuleList([\n",
        "            StyleBlock(512, 512, latent_dim) for _ in range(n_styles)\n",
        "        ])\n",
        "\n",
        "        # To RGB conversion at each resolution\n",
        "        self.to_rgbs = nn.ModuleList([\n",
        "            EqualizedConv2d(512, img_channels, kernel_size=1)\n",
        "            for _ in range(n_styles)\n",
        "        ])\n",
        "\n",
        "        # Upsampling\n",
        "        self.upsampler = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "    def forward(self, style, alpha=1.0):\n",
        "        # Initial constant input\n",
        "        x = self.initial_const.repeat(style.size(0), 1, 1, 1)\n",
        "\n",
        "        # Progressive style application\n",
        "        rgb_outputs = []\n",
        "        for i, (style_block, to_rgb) in enumerate(zip(self.style_blocks, self.to_rgbs)):\n",
        "            # Apply style\n",
        "            x = style_block(x, style[:, i])\n",
        "\n",
        "            # Upsample except for first block\n",
        "            if i > 0:\n",
        "                x = self.upsampler(x)\n",
        "\n",
        "            # Convert to RGB\n",
        "            rgb = to_rgb(x)\n",
        "            rgb_outputs.append(rgb)\n",
        "\n",
        "        # Blend resolutions during training\n",
        "        if len(rgb_outputs) > 1 and alpha < 1.0:\n",
        "            return (1 - alpha) * rgb_outputs[-2] + alpha * rgb_outputs[-1]\n",
        "\n",
        "        return rgb_outputs[-1]\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels=3, dropout_rate=0.4):\n",
        "        super().__init__()\n",
        "\n",
        "        # Progressive growing discriminator blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                EqualizedConv2d(img_channels, 64, kernel_size=1),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Dropout2d(p=dropout_rate),\n",
        "                EqualizedConv2d(64, 64, kernel_size=3, padding=1),\n",
        "                nn.LeakyReLU(0.2)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                EqualizedConv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Dropout2d(p=dropout_rate),\n",
        "                EqualizedConv2d(128, 128, kernel_size=3, padding=1),\n",
        "                nn.LeakyReLU(0.2)\n",
        "            ),\n",
        "            # Additional blocks can be added for higher resolutions\n",
        "        ])\n",
        "\n",
        "        # Final classification layer\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(128 * 4 * 4, 1),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def forward(self, x, alpha=1.0):\n",
        "        # Progressive growing\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "\n",
        "            # Optional alpha blending between resolutions\n",
        "            if i > 0 and alpha < 1.0:\n",
        "                x_prev = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "                x = (1 - alpha) * x_prev + alpha * x\n",
        "\n",
        "        return self.final_layer(x)\n",
        "\n",
        "    def print_configuration(self):\n",
        "        \"\"\"Print detailed configuration of the Discriminator.\"\"\"\n",
        "        print(\"Discriminator Configuration:\")\n",
        "        print(f\"Dropout Rate: {self.dropout_rate}\")\n",
        "        print(\"Block Configurations:\")\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            print(f\"  Block {i+1}:\")\n",
        "            for layer in block:\n",
        "                if isinstance(layer, EqualizedConv2d):\n",
        "                    print(f\"    Equalized Conv2d: {layer.weight.shape}\")\n",
        "                elif isinstance(layer, nn.LeakyReLU):\n",
        "                    print(f\"    LeakyReLU: Negative Slope = 0.2\")\n",
        "                elif isinstance(layer, nn.Dropout2d):\n",
        "                    print(f\"    Dropout2d: Rate = {layer.p}\")\n",
        "\n",
        "        print(\"Final Classification Layer:\")\n",
        "        for layer in self.final_layer:\n",
        "            if isinstance(layer, nn.Dropout):\n",
        "                print(f\"  Dropout: Rate = {layer.p}\")\n",
        "            elif isinstance(layer, nn.Linear):\n",
        "                print(f\"  Linear: {layer.in_features} → {layer.out_features}\")\n",
        "\n",
        "# Example usage\n",
        "def generate_high_res_images():\n",
        "    # Instantiate generator\n",
        "    generator = Generator()\n",
        "\n",
        "    # Generate random latent vectors\n",
        "    latent_vectors = torch.randn(4, 18, 512)  # 4 images, 18 style layers\n",
        "\n",
        "    # Generate images\n",
        "    generated_images = generator(latent_vectors)\n",
        "\n",
        "    return generated_images\n",
        "\n",
        "# Training configuration example\n",
        "class StyleGANTrainer:\n",
        "    def __init__(self, lr=0.001):\n",
        "        self.generator = Generator()\n",
        "        self.discriminator = Discriminator()\n",
        "\n",
        "        # Optimizers\n",
        "        self.g_optimizer = torch.optim.Adam(\n",
        "            self.generator.parameters(),\n",
        "            lr=lr,\n",
        "            betas=(0.0, 0.99)\n",
        "        )\n",
        "        self.d_optimizer = torch.optim.Adam(\n",
        "            self.discriminator.parameters(),\n",
        "            lr=lr,\n",
        "            betas=(0.0, 0.99)\n",
        "        )\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Generate fake images\n",
        "        z = torch.randn(real_images.size(0), 18, 512)\n",
        "        fake_images = self.generator(z)\n",
        "\n",
        "        # Discriminator forward pass\n",
        "        real_scores = self.discriminator(real_images)\n",
        "        fake_scores = self.discriminator(fake_images)\n",
        "\n",
        "        # Adversarial loss\n",
        "        d_loss = F.softplus(fake_scores) + F.softplus(-real_scores)\n",
        "        g_loss = F.softplus(-fake_scores)\n",
        "\n",
        "        # Backpropagation\n",
        "        self.d_optimizer.zero_grad()\n",
        "        d_loss.backward()\n",
        "        self.d_optimizer.step()\n",
        "\n",
        "        self.g_optimizer.zero_grad()\n",
        "        g_loss.backward()\n",
        "        self.g_optimizer.step()\n",
        "\n",
        "        return d_loss.item(), g_loss.item()\n",
        "\n",
        "# Advanced features demonstration\n",
        "def style_mixing_experiment():\n",
        "    generator = Generator()\n",
        "\n",
        "    # Generate two different latent vectors\n",
        "    z1 = torch.randn(1, 18, 512)\n",
        "    z2 = torch.randn(1, 18, 512)\n",
        "\n",
        "    # Style mixing: use first half of styles from z1, second from z2\n",
        "    mixed_z = torch.cat([z1[:, :9], z2[:, 9:]], dim=1)\n",
        "\n",
        "    # Generate image with mixed styles\n",
        "    mixed_image = generator(mixed_z)\n",
        "\n",
        "    return mixed_image\n",
        "\n",
        "# Advanced feature: noise injection\n",
        "class NoiseInjection(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
        "\n",
        "    def forward(self, x, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device)\n",
        "        return x + self.weight * noise"
      ],
      "metadata": {
        "id": "aUFYQwppKChB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a function to modify the discriminator to include a dropout layer with a rate of 0.4 and print the configurations."
      ],
      "metadata": {
        "id": "EG2CcVbZKIJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def modify_discriminator_with_dropout(discriminator, dropout_rate=0.4):\n",
        "    \"\"\"\n",
        "    Modifies the given discriminator model to include a dropout layer with the specified rate.\n",
        "    The dropout is added after each linear or convolutional layer in the model.\n",
        "\n",
        "    Args:\n",
        "        discriminator (nn.Module): The discriminator model to modify.\n",
        "        dropout_rate (float): The rate for the dropout layer (default is 0.4).\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The modified discriminator model.\n",
        "    \"\"\"\n",
        "    # Create a new sequential model to include dropout layers\n",
        "    modified_layers = []\n",
        "    for name, layer in discriminator.named_children():\n",
        "        # Add the original layer\n",
        "        modified_layers.append((name, layer))\n",
        "\n",
        "        # Add a dropout layer after linear or convolutional layers\n",
        "        if isinstance(layer, (nn.Linear, nn.Conv2d)):\n",
        "            modified_layers.append((f\"{name}_dropout\", nn.Dropout(p=dropout_rate)))\n",
        "\n",
        "    # Construct a new model with the modified layers\n",
        "    modified_discriminator = nn.Sequential(nn.ModuleDict(modified_layers))\n",
        "    return modified_discriminator\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define a simple discriminator\n",
        "    class SimpleDiscriminator(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(SimpleDiscriminator, self).__init__()\n",
        "            self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
        "            self.relu1 = nn.ReLU()\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "            self.relu2 = nn.ReLU()\n",
        "            self.fc = nn.Linear(64 * 7 * 7, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.conv1(x)\n",
        "            x = self.relu1(x)\n",
        "            x = self.conv2(x)\n",
        "            x = self.relu2(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.fc(x)\n",
        "            return x\n",
        "\n",
        "    # Instantiate the discriminator\n",
        "    original_discriminator = SimpleDiscriminator()\n",
        "    print(\"Original Discriminator:\")\n",
        "    print(original_discriminator)\n",
        "\n",
        "    # Modify the discriminator to include dropout\n",
        "    modified_discriminator = modify_discriminator_with_dropout(original_discriminator)\n",
        "    print(\"\\nModified Discriminator with Dropout:\")\n",
        "    print(modified_discriminator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2GQUGFXKNaj",
        "outputId": "2e5f155e-b5f2-4cc0-a651-9e064ac56f54"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Discriminator:\n",
            "SimpleDiscriminator(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (fc): Linear(in_features=3136, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Modified Discriminator with Dropout:\n",
            "Sequential(\n",
            "  (0): ModuleDict(\n",
            "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (conv1_dropout): Dropout(p=0.4, inplace=False)\n",
            "    (relu1): ReLU()\n",
            "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (conv2_dropout): Dropout(p=0.4, inplace=False)\n",
            "    (relu2): ReLU()\n",
            "    (fc): Linear(in_features=3136, out_features=1, bias=True)\n",
            "    (fc_dropout): Dropout(p=0.4, inplace=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KrxAlo40KU5w"
      }
    }
  ]
}